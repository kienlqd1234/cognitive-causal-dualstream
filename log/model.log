2025-01-25 15:49:50,215 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-25 15:49:50,230 INFO Start graph assembling...
2025-01-26 16:47:56,941 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 16:47:56,949 INFO Start graph assembling...
2025-01-26 16:56:40,984 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 16:56:40,984 INFO Start graph assembling...
2025-01-26 16:58:48,777 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 16:58:48,787 INFO Start graph assembling...
2025-01-26 17:02:36,263 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 17:02:36,267 INFO Start graph assembling...
2025-01-26 17:04:08,005 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 17:04:08,005 INFO Start graph assembling...
2025-01-26 17:04:31,387 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 17:06:05,373 INFO Word table init: done!
2025-01-26 17:06:05,373 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 17:06:05,373 INFO Epoch: 1/10 start
2025-01-26 22:13:30,321 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 22:13:30,321 INFO Start graph assembling...
2025-01-26 22:13:53,801 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 22:13:55,219 INFO Word table init: done!
2025-01-26 22:13:55,219 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 22:13:55,219 INFO Epoch: 1/10 start
2025-01-26 22:40:14,160 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 22:40:14,160 INFO Start graph assembling...
2025-01-26 22:40:36,366 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 22:40:37,328 INFO Word table init: done!
2025-01-26 22:40:37,328 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 22:40:37,343 INFO Epoch: 1/10 start
2025-01-26 23:06:09,879 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 23:06:09,879 INFO Start graph assembling...
2025-01-26 23:06:32,596 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 23:06:33,925 INFO Word table init: done!
2025-01-26 23:06:33,925 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 23:06:33,925 INFO Epoch: 1/10 start
2025-01-26 23:07:20,048 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 23:07:20,048 INFO Start graph assembling...
2025-01-26 23:07:45,091 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 23:07:46,088 INFO Word table init: done!
2025-01-26 23:07:46,088 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 23:07:46,090 INFO Epoch: 1/10 start
2025-01-26 23:09:42,764 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 23:09:42,764 INFO Start graph assembling...
2025-01-26 23:11:32,472 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 23:11:32,472 INFO Start graph assembling...
2025-01-26 23:11:55,260 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 23:11:56,287 INFO Word table init: done!
2025-01-26 23:11:56,287 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 23:11:56,287 INFO Epoch: 1/10 start
2025-01-26 23:54:50,666 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-26 23:54:50,666 INFO Start graph assembling...
2025-01-26 23:55:14,963 INFO ASSEMBLE: word table #replacement: 19482
2025-01-26 23:55:16,000 INFO Word table init: done!
2025-01-26 23:55:16,000 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-26 23:55:16,000 INFO Epoch: 1/10 start
2025-01-27 00:02:47,426 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-27 00:02:47,426 INFO Start graph assembling...
2025-01-27 00:03:10,313 INFO ASSEMBLE: word table #replacement: 19482
2025-01-27 00:03:11,410 INFO Word table init: done!
2025-01-27 00:03:11,410 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-27 00:03:11,419 INFO Epoch: 1/10 start
2025-01-27 00:03:11,424 ERROR File ./data/price/preprocessed\tech.txt not found.
2025-01-27 00:03:11,424 ERROR File ./data/price/preprocessed\healthcare.txt not found.
2025-01-27 00:03:11,425 ERROR File ./data/price/preprocessed\utilities.txt not found.
2025-01-27 00:03:11,425 ERROR File ./data/price/preprocessed\consumer_goods.txt not found.
2025-01-27 00:03:11,426 ERROR File ./data/price/preprocessed\industrial_goods.txt not found.
2025-01-27 00:03:11,426 ERROR File ./data/price/preprocessed\finance.txt not found.
2025-01-27 00:03:11,426 ERROR File ./data/price/preprocessed\cong.txt not found.
2025-01-27 00:03:11,427 ERROR File ./data/price/preprocessed\materials.txt not found.
2025-01-27 00:03:11,427 ERROR File ./data/price/preprocessed\services.txt not found.
2025-01-27 00:09:11,227 INFO INIT: #stock: 9, #vocab+1: 29867
2025-01-27 00:09:11,227 INFO Start graph assembling...
2025-01-27 00:09:34,566 INFO ASSEMBLE: word table #replacement: 19482
2025-01-27 00:09:35,900 INFO Word table init: done!
2025-01-27 00:09:35,900 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-27 00:09:35,900 INFO Epoch: 1/10 start
2025-01-27 00:09:35,900 ERROR File ./data/price/preprocessed\tech.txt not found.
2025-01-27 00:09:35,910 ERROR File ./data/price/preprocessed\cong.txt not found.
2025-01-27 00:09:35,910 ERROR File ./data/price/preprocessed\healthcare.txt not found.
2025-01-27 00:09:35,910 ERROR File ./data/price/preprocessed\industrial_goods.txt not found.
2025-01-27 00:09:35,911 ERROR File ./data/price/preprocessed\services.txt not found.
2025-01-27 00:09:35,911 ERROR File ./data/price/preprocessed\finance.txt not found.
2025-01-27 00:09:35,912 ERROR File ./data/price/preprocessed\utilities.txt not found.
2025-01-27 00:09:35,912 ERROR File ./data/price/preprocessed\consumer_goods.txt not found.
2025-01-27 00:09:35,912 ERROR File ./data/price/preprocessed\materials.txt not found.
2025-01-27 00:10:08,858 INFO INIT: #stock: 88, #vocab+1: 29867
2025-01-27 00:10:08,864 INFO Start graph assembling...
2025-01-27 00:10:31,721 INFO ASSEMBLE: word table #replacement: 19482
2025-01-27 00:10:33,041 INFO Word table init: done!
2025-01-27 00:10:33,041 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-27 00:10:33,041 INFO Epoch: 1/10 start
2025-01-27 00:19:11,912 INFO 	iter: 20, batch loss: 7.7886881828308105, batch acc: 0.562500
2025-01-27 10:14:47,961 INFO INIT: #stock: 88, #vocab+1: 29867
2025-01-27 10:14:47,961 INFO Start graph assembling...
2025-01-27 10:15:10,756 INFO ASSEMBLE: word table #replacement: 19482
2025-01-27 10:15:12,227 INFO Word table init: done!
2025-01-27 10:15:12,243 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, start a new session!
2025-01-27 10:15:12,243 INFO Epoch: 1/10 start
2025-01-27 10:22:12,477 INFO 	iter: 20, batch loss: 8.26638126373291, batch acc: 0.562500
2025-01-27 10:27:46,717 INFO 	iter: 40, batch loss: 6.986863136291504, batch acc: 0.562500
2025-01-27 10:33:23,673 INFO 	iter: 60, batch loss: 4.4614410400390625, batch acc: 0.656250
2025-01-27 10:38:59,179 INFO 	iter: 80, batch loss: 3.090616226196289, batch acc: 0.656250
2025-01-27 10:44:31,302 INFO 	iter: 100, batch loss: 2.792022228240967, batch acc: 0.468750
2025-01-27 10:49:59,338 INFO 	iter: 120, batch loss: 2.838326930999756, batch acc: 0.687500
2025-01-27 10:55:20,598 INFO 	iter: 140, batch loss: 2.7194721698760986, batch acc: 0.562500
2025-01-27 10:59:15,368 INFO Epoch: loss: 4.275567, acc: 0.605444
2025-01-27 10:59:15,369 INFO Epoch: 2/10 start
2025-01-27 11:01:27,859 INFO 	iter: 160, batch loss: 2.4171440601348877, batch acc: 0.531250
2025-01-27 11:07:36,317 INFO 	iter: 180, batch loss: 2.295459747314453, batch acc: 0.500000
2025-01-27 11:13:04,775 INFO 	iter: 200, batch loss: 2.153496742248535, batch acc: 0.718750
2025-01-27 11:18:33,244 INFO 	iter: 220, batch loss: 1.9103819131851196, batch acc: 0.718750
2025-01-27 11:23:49,305 INFO 	iter: 240, batch loss: 1.8949658870697021, batch acc: 0.625000
2025-01-27 11:29:04,259 INFO 	iter: 260, batch loss: 1.836007833480835, batch acc: 0.468750
2025-02-13 11:38:25,807 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-13 11:38:25,807 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-13 11:50:02,461 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-13 11:50:02,461 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-19 12:51:10,865 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-19 12:51:10,871 INFO Start graph assembling...
2025-02-19 12:51:34,061 INFO ASSEMBLE: word table #replacement: 19482
2025-02-19 12:51:36,020 INFO Word table init: done!
2025-02-19 12:51:36,932 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-19 12:51:36,933 INFO Epoch: 1/10 start
2025-02-19 12:58:47,166 INFO 	iter: 280, batch loss: 1.6174944639205933, batch acc: 0.625000
2025-02-19 13:04:56,382 INFO 	iter: 300, batch loss: 1.6333199739456177, batch acc: 0.437500
2025-02-19 13:10:58,635 INFO 	iter: 320, batch loss: 1.607727289199829, batch acc: 0.437500
2025-02-25 11:45:57,783 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:45:57,783 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:45:57,788 INFO Start graph assembling...
2025-02-25 11:45:57,788 INFO Start graph assembling...
2025-02-25 11:47:30,159 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:47:30,159 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:47:30,162 INFO Start graph assembling...
2025-02-25 11:47:30,162 INFO Start graph assembling...
2025-02-25 11:52:56,488 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:52:56,488 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:52:56,491 INFO Start graph assembling...
2025-02-25 11:52:56,491 INFO Start graph assembling...
2025-02-25 11:55:09,105 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:55:09,105 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:55:09,108 INFO Start graph assembling...
2025-02-25 11:55:09,108 INFO Start graph assembling...
2025-02-25 11:57:08,042 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:57:08,042 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:57:08,044 INFO Start graph assembling...
2025-02-25 11:57:08,044 INFO Start graph assembling...
2025-02-25 11:58:34,854 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:58:34,854 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 11:58:34,856 INFO Start graph assembling...
2025-02-25 11:58:34,856 INFO Start graph assembling...
2025-02-25 12:00:37,659 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:00:37,659 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:00:37,662 INFO Start graph assembling...
2025-02-25 12:00:37,662 INFO Start graph assembling...
2025-02-25 12:01:55,320 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:01:55,320 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:01:55,323 INFO Start graph assembling...
2025-02-25 12:01:55,323 INFO Start graph assembling...
2025-02-25 12:06:32,677 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:06:32,677 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 12:06:32,679 INFO Start graph assembling...
2025-02-25 12:06:32,679 INFO Start graph assembling...
2025-02-25 12:06:55,882 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 12:06:55,882 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 12:06:57,869 INFO Word table init: done!
2025-02-25 12:06:57,869 INFO Word table init: done!
2025-02-25 12:06:58,854 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 12:06:58,854 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 12:06:58,855 INFO Epoch: 1/10 start
2025-02-25 12:06:58,855 INFO Epoch: 1/10 start
2025-02-25 13:15:39,017 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:15:39,017 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:15:39,019 INFO Start graph assembling...
2025-02-25 13:15:39,019 INFO Start graph assembling...
2025-02-25 13:16:00,988 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:16:00,988 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:16:02,589 INFO Word table init: done!
2025-02-25 13:16:02,589 INFO Word table init: done!
2025-02-25 13:16:03,574 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:16:03,574 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:16:03,575 INFO Epoch: 1/10 start
2025-02-25 13:16:03,575 INFO Epoch: 1/10 start
2025-02-25 13:49:13,486 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:49:13,486 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:49:13,488 INFO Start graph assembling...
2025-02-25 13:49:13,488 INFO Start graph assembling...
2025-02-25 13:49:35,321 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:49:35,321 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:49:36,570 INFO Word table init: done!
2025-02-25 13:49:36,570 INFO Word table init: done!
2025-02-25 13:49:37,554 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:49:37,554 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:49:37,554 INFO Epoch: 1/10 start
2025-02-25 13:49:37,554 INFO Epoch: 1/10 start
2025-02-25 13:58:13,153 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:58:13,153 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-25 13:58:13,156 INFO Start graph assembling...
2025-02-25 13:58:13,156 INFO Start graph assembling...
2025-02-25 13:58:35,060 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:58:35,060 INFO ASSEMBLE: word table #replacement: 19482
2025-02-25 13:58:36,247 INFO Word table init: done!
2025-02-25 13:58:36,247 INFO Word table init: done!
2025-02-25 13:58:37,244 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:58:37,244 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-02-25 13:58:37,244 INFO Epoch: 1/10 start
2025-02-25 13:58:37,244 INFO Epoch: 1/10 start
2025-02-25 14:06:18,360 INFO 	iter: 340, batch loss: 1.4083565473556519, batch acc: 0.625000
2025-02-25 14:06:18,360 INFO 	iter: 340, batch loss: 1.4083565473556519, batch acc: 0.625000
2025-02-25 14:12:36,177 INFO 	iter: 360, batch loss: 1.3991094827651978, batch acc: 0.500000
2025-02-25 14:12:36,177 INFO 	iter: 360, batch loss: 1.3991094827651978, batch acc: 0.500000
2025-02-25 14:18:41,830 INFO 	iter: 380, batch loss: 1.3835928440093994, batch acc: 0.531250
2025-02-25 14:18:41,830 INFO 	iter: 380, batch loss: 1.3835928440093994, batch acc: 0.531250
2025-02-25 14:24:47,471 INFO 	iter: 400, batch loss: 1.314979910850525, batch acc: 0.531250
2025-02-25 14:24:47,471 INFO 	iter: 400, batch loss: 1.314979910850525, batch acc: 0.531250
2025-02-25 14:30:57,903 INFO 	iter: 420, batch loss: 1.2810200452804565, batch acc: 0.625000
2025-02-25 14:30:57,903 INFO 	iter: 420, batch loss: 1.2810200452804565, batch acc: 0.625000
2025-02-25 14:36:53,560 INFO 	iter: 440, batch loss: 1.300082802772522, batch acc: 0.593750
2025-02-25 14:36:53,560 INFO 	iter: 440, batch loss: 1.300082802772522, batch acc: 0.593750
2025-02-25 14:42:43,599 INFO 	iter: 460, batch loss: 1.1715271472930908, batch acc: 0.750000
2025-02-25 14:42:43,599 INFO 	iter: 460, batch loss: 1.1715271472930908, batch acc: 0.750000
2025-02-25 14:47:03,076 INFO Epoch: loss: 1.372381, acc: 0.550403
2025-02-25 14:47:03,076 INFO Epoch: loss: 1.372381, acc: 0.550403
2025-02-25 14:47:03,076 INFO Epoch: 2/10 start
2025-02-25 14:47:03,076 INFO Epoch: 2/10 start
2025-02-25 14:49:21,603 INFO 	iter: 480, batch loss: 1.2081903219223022, batch acc: 0.593750
2025-02-25 14:49:21,603 INFO 	iter: 480, batch loss: 1.2081903219223022, batch acc: 0.593750
2025-02-26 12:31:14,226 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:31:14,226 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:31:14,233 INFO Start graph assembling...
2025-02-26 12:31:14,233 INFO Start graph assembling...
2025-02-26 12:33:46,464 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:33:46,464 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:33:46,466 INFO Start graph assembling...
2025-02-26 12:33:46,466 INFO Start graph assembling...
2025-02-26 12:35:32,980 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:35:32,980 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:35:32,983 INFO Start graph assembling...
2025-02-26 12:35:32,983 INFO Start graph assembling...
2025-02-26 12:39:47,270 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:39:47,270 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:39:47,272 INFO Start graph assembling...
2025-02-26 12:39:47,272 INFO Start graph assembling...
2025-02-26 12:42:46,278 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:42:46,278 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:42:46,281 INFO Start graph assembling...
2025-02-26 12:42:46,281 INFO Start graph assembling...
2025-02-26 12:43:33,136 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:43:33,136 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-26 12:43:33,139 INFO Start graph assembling...
2025-02-26 12:43:33,139 INFO Start graph assembling...
2025-02-27 10:35:52,660 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:35:52,660 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:35:52,660 INFO Start graph assembling...
2025-02-27 10:35:52,660 INFO Start graph assembling...
2025-02-27 10:36:47,634 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:36:47,634 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:36:47,634 INFO Start graph assembling...
2025-02-27 10:36:47,634 INFO Start graph assembling...
2025-02-27 10:56:39,045 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:56:39,045 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 10:56:39,061 INFO Start graph assembling...
2025-02-27 10:56:39,061 INFO Start graph assembling...
2025-02-27 11:04:34,436 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:04:34,436 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:04:34,452 INFO Start graph assembling...
2025-02-27 11:04:34,452 INFO Start graph assembling...
2025-02-27 11:06:21,851 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:06:21,851 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:06:21,851 INFO Start graph assembling...
2025-02-27 11:06:21,851 INFO Start graph assembling...
2025-02-27 11:14:22,682 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:14:22,682 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:14:22,682 INFO Start graph assembling...
2025-02-27 11:14:22,682 INFO Start graph assembling...
2025-02-27 11:14:49,991 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:14:49,991 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:14:50,007 INFO Start graph assembling...
2025-02-27 11:14:50,007 INFO Start graph assembling...
2025-02-27 11:53:23,047 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:53:23,047 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:53:23,058 INFO Start graph assembling...
2025-02-27 11:53:23,058 INFO Start graph assembling...
2025-02-27 11:56:26,321 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:56:26,321 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 11:56:26,321 INFO Start graph assembling...
2025-02-27 11:56:26,321 INFO Start graph assembling...
2025-02-27 12:18:46,832 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:18:46,832 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:18:46,848 INFO Start graph assembling...
2025-02-27 12:18:46,848 INFO Start graph assembling...
2025-02-27 12:19:26,497 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:19:26,497 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:19:26,497 INFO Start graph assembling...
2025-02-27 12:19:26,497 INFO Start graph assembling...
2025-02-27 12:31:49,142 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:31:49,142 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 12:31:49,142 INFO Start graph assembling...
2025-02-27 12:31:49,142 INFO Start graph assembling...
2025-02-27 13:30:57,665 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 13:30:57,665 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 13:30:57,665 INFO Start graph assembling...
2025-02-27 13:30:57,665 INFO Start graph assembling...
2025-02-27 13:32:43,598 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 13:32:43,598 INFO INIT: #stock: 88, #vocab+1: 29867
2025-02-27 13:32:43,598 INFO Start graph assembling...
2025-02-27 13:32:43,598 INFO Start graph assembling...
2025-04-14 08:47:31,486 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 08:47:31,486 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 08:47:31,486 INFO Start graph assembling...
2025-04-14 08:47:31,486 INFO Start graph assembling...
2025-04-14 09:02:55,439 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:02:55,439 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:02:55,441 INFO Start graph assembling...
2025-04-14 09:02:55,441 INFO Start graph assembling...
2025-04-14 09:06:59,882 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:06:59,882 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:06:59,884 INFO Start graph assembling...
2025-04-14 09:06:59,884 INFO Start graph assembling...
2025-04-14 09:13:30,993 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:13:30,993 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:13:30,996 INFO Start graph assembling...
2025-04-14 09:13:30,996 INFO Start graph assembling...
2025-04-14 09:25:30,402 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:25:30,402 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:25:30,404 INFO Start graph assembling...
2025-04-14 09:25:30,404 INFO Start graph assembling...
2025-04-14 09:32:10,475 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:32:10,475 INFO INIT: #stock: 88, #vocab+1: 29867
2025-04-14 09:32:10,477 INFO Start graph assembling...
2025-04-14 09:32:10,477 INFO Start graph assembling...
2025-05-12 15:18:39,801 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-12 15:18:39,801 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-12 15:18:39,807 INFO Start graph assembling...
2025-05-12 15:18:39,807 INFO Start graph assembling...
2025-05-12 15:19:01,225 INFO ASSEMBLE: word table #replacement: 19482
2025-05-12 15:19:01,225 INFO ASSEMBLE: word table #replacement: 19482
2025-05-12 15:20:47,874 INFO Word table init: done!
2025-05-12 15:20:47,874 INFO Word table init: done!
2025-05-12 15:20:49,051 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-05-12 15:20:49,051 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-05-12 15:20:49,052 INFO Epoch: 1/10 start
2025-05-12 15:20:49,052 INFO Epoch: 1/10 start
2025-05-12 15:29:27,680 INFO 	iter: 500, batch loss: 1.2450776100158691, batch acc: 0.531250
2025-05-12 15:29:27,680 INFO 	iter: 500, batch loss: 1.2450776100158691, batch acc: 0.531250
2025-05-16 15:44:36,432 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 15:44:36,432 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 15:44:52,502 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 15:44:52,502 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 16:02:59,984 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:02:59,984 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:02:59,987 INFO Start graph assembling...
2025-05-16 16:02:59,987 INFO Start graph assembling...
2025-05-16 16:22:30,189 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:22:30,189 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:22:30,192 INFO Start graph assembling...
2025-05-16 16:22:30,192 INFO Start graph assembling...
2025-05-16 16:37:54,782 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:37:54,782 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:37:54,786 INFO Start graph assembling...
2025-05-16 16:37:54,786 INFO Start graph assembling...
2025-05-16 16:41:09,724 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:41:09,724 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 16:41:09,727 INFO Start graph assembling...
2025-05-16 16:41:09,727 INFO Start graph assembling...
2025-05-16 17:12:18,682 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 17:12:18,682 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 17:12:18,691 INFO Start graph assembling...
2025-05-16 17:12:18,691 INFO Start graph assembling...
2025-05-16 17:12:39,747 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 17:12:39,747 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 17:15:39,343 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 17:15:39,343 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 17:15:39,354 INFO Start graph assembling...
2025-05-16 17:15:39,354 INFO Start graph assembling...
2025-05-16 17:16:00,124 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 17:16:00,124 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 22:49:02,589 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 22:49:02,589 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-16 22:49:02,658 INFO Start graph assembling...
2025-05-16 22:49:02,658 INFO Start graph assembling...
2025-05-16 22:49:23,162 INFO ASSEMBLE: word table #replacement: 19482
2025-05-16 22:49:23,162 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 11:24:29,657 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 11:24:29,657 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 11:24:29,663 INFO Start graph assembling...
2025-05-25 11:24:29,663 INFO Start graph assembling...
2025-05-25 11:25:09,812 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 11:25:09,812 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 11:25:09,814 INFO Start graph assembling...
2025-05-25 11:25:09,814 INFO Start graph assembling...
2025-05-25 11:25:31,630 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 11:25:31,630 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 12:01:42,184 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 12:01:42,184 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 12:01:42,187 INFO Start graph assembling...
2025-05-25 12:01:42,187 INFO Start graph assembling...
2025-05-25 12:02:03,456 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 12:02:03,456 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 12:33:03,183 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 12:33:03,183 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 12:33:03,187 INFO Start graph assembling...
2025-05-25 12:33:03,187 INFO Start graph assembling...
2025-05-25 12:33:24,108 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 12:33:24,108 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:16:03,400 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:16:03,400 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:16:03,403 INFO Start graph assembling...
2025-05-25 13:16:03,403 INFO Start graph assembling...
2025-05-25 13:16:25,218 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:16:25,218 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:51:26,279 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:51:26,279 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:51:26,282 INFO Start graph assembling...
2025-05-25 13:51:26,282 INFO Start graph assembling...
2025-05-25 13:51:49,056 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:51:49,056 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:58:09,363 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:58:09,363 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:58:09,365 INFO Start graph assembling...
2025-05-25 13:58:09,365 INFO Start graph assembling...
2025-05-25 13:58:34,067 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:58:34,067 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:59:17,325 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:59:17,325 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 13:59:17,328 INFO Start graph assembling...
2025-05-25 13:59:17,328 INFO Start graph assembling...
2025-05-25 13:59:39,076 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 13:59:39,076 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:01:56,925 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:01:56,925 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:01:56,928 INFO Start graph assembling...
2025-05-25 14:01:56,928 INFO Start graph assembling...
2025-05-25 14:02:20,789 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:02:20,789 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:05:49,457 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:05:49,457 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:05:49,461 INFO Start graph assembling...
2025-05-25 14:05:49,461 INFO Start graph assembling...
2025-05-25 14:06:14,651 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:06:14,651 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:08:32,226 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:08:32,226 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:08:32,229 INFO Start graph assembling...
2025-05-25 14:08:32,229 INFO Start graph assembling...
2025-05-25 14:09:08,534 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:09:08,534 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:16:23,990 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:16:23,990 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 14:16:23,993 INFO Start graph assembling...
2025-05-25 14:16:23,993 INFO Start graph assembling...
2025-05-25 14:16:47,814 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 14:16:47,814 INFO ASSEMBLE: word table #replacement: 19482
2025-05-25 15:31:22,992 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:31:22,992 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:35:12,464 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:35:12,464 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:35:12,467 INFO Start graph assembling...
2025-05-25 15:35:12,467 INFO Start graph assembling...
2025-05-25 15:37:34,135 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:37:34,135 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:37:34,137 INFO Start graph assembling...
2025-05-25 15:37:34,137 INFO Start graph assembling...
2025-05-25 15:39:58,071 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:39:58,071 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 15:39:58,073 INFO Start graph assembling...
2025-05-25 15:39:58,073 INFO Start graph assembling...
2025-05-25 16:10:32,495 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:10:32,495 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:10:32,497 INFO Start graph assembling...
2025-05-25 16:10:32,497 INFO Start graph assembling...
2025-05-25 16:15:39,520 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:15:39,520 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:15:39,523 INFO Start graph assembling...
2025-05-25 16:15:39,523 INFO Start graph assembling...
2025-05-25 16:18:23,776 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:18:23,776 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:18:23,779 INFO Start graph assembling...
2025-05-25 16:18:23,779 INFO Start graph assembling...
2025-05-25 16:23:13,082 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:23:13,082 INFO INIT: #stock: 88, #vocab+1: 29867
2025-05-25 16:23:13,085 INFO Start graph assembling...
2025-05-25 16:23:13,085 INFO Start graph assembling...
2025-05-26 18:32:02,686 INFO Initializing MeaningAwareSelection for testing...
2025-05-26 18:32:02,687 INFO MeaningAwareSelection initialized successfully.
2025-05-26 18:32:02,687 INFO 
--- Testing filter for stock: AAPL ---
2025-05-26 18:32:02,688 INFO Original texts:
2025-05-26 18:32:02,688 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-26 18:32:02,688 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-26 18:32:02,688 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-26 18:32:02,689 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-26 18:32:02,689 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-26 18:32:02,689 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-26 18:32:02,690 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-26 18:32:02,690 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-26 18:32:02,690 ERROR Error during text filtering: filter_texts() got an unexpected keyword argument 'stock_symbol'
2025-05-26 18:37:48,400 INFO Initializing MeaningAwareSelection for testing...
2025-05-26 18:37:48,401 INFO MeaningAwareSelection initialized successfully.
2025-05-26 18:37:48,401 INFO 
--- Testing filter for stock: AAPL ---
2025-05-26 18:37:48,402 INFO Original texts:
2025-05-26 18:37:48,402 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-26 18:37:48,402 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-26 18:37:48,403 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-26 18:37:48,403 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-26 18:37:48,403 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-26 18:37:48,403 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-26 18:37:48,404 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-26 18:37:48,404 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-26 18:37:52,444 INFO 
--- Filter Results ---
2025-05-26 18:37:52,445 INFO Found 0 RELEVANT texts for AAPL:
2025-05-26 18:37:52,446 INFO   None
2025-05-26 18:37:52,446 INFO 
Found 8 IRRELEVANT texts for AAPL:
2025-05-26 18:37:52,447 INFO   IR1. False
2025-05-26 18:37:52,447 INFO   IR2. False
2025-05-26 18:37:52,447 INFO   IR3. False
2025-05-26 18:37:52,448 INFO   IR4. False
2025-05-26 18:37:52,448 INFO   IR5. False
2025-05-26 18:37:52,448 INFO   IR6. False
2025-05-26 18:37:52,448 INFO   IR7. False
2025-05-26 18:37:52,449 INFO   IR8. False
2025-05-26 18:37:52,449 INFO 
--- Test Complete ---
2025-05-26 18:48:42,551 INFO Initializing MeaningAwareSelection for testing...
2025-05-26 18:48:42,554 INFO Using LLM: OpenAILLM with model gpt-3.5-turbo
2025-05-26 18:48:42,554 INFO MeaningAwareSelection initialized successfully.
2025-05-26 18:48:42,555 INFO 
--- Testing filter for stock: AAPL ---
2025-05-26 18:48:42,555 INFO Original texts:
2025-05-26 18:48:42,556 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-26 18:48:42,556 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-26 18:48:42,556 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-26 18:48:42,556 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-26 18:48:42,557 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-26 18:48:42,557 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-26 18:48:42,557 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-26 18:48:42,557 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-26 18:48:45,581 ERROR Error during text filtering: module 'openai' has no attribute 'ChatCompletion'
2025-05-27 11:05:19,129 INFO Initializing MeaningAwareSelection for testing...
2025-05-27 11:05:19,130 INFO Using LLM: OpenAILLM with model gpt-3.5-turbo
2025-05-27 11:05:19,130 INFO MeaningAwareSelection initialized successfully.
2025-05-27 11:05:19,130 INFO 
--- Testing filter for stock: AAPL ---
2025-05-27 11:05:19,131 INFO Original texts:
2025-05-27 11:05:19,131 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 11:05:19,131 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-27 11:05:19,131 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 11:05:19,132 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 11:05:19,132 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-27 11:05:19,132 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 11:05:19,132 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 11:05:19,133 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-27 11:05:22,151 ERROR Error during text filtering: module 'openai' has no attribute 'ChatCompletion'
2025-05-27 11:55:10,092 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 11:55:10,093 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 11:55:12,342 ERROR Failed to initialize LLM interface: Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file


2025-05-27 12:05:57,104 INFO SentencePiece library found.
2025-05-27 12:05:57,105 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:05:57,105 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:05:57,106 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:06:47,280 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:06:47,281 INFO Attempting to load primary NLI model (microsoft/deberta-v3-small-mnli)...
2025-05-27 12:06:48,339 ERROR Failed to initialize primary NLI LLM interface (microsoft/deberta-v3-small-mnli): Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file


2025-05-27 12:06:48,340 ERROR Please ensure you have an active internet connection for the first download, and that 'sentencepiece' is installed (`pip install sentencepiece`).
2025-05-27 12:06:48,341 ERROR If the problem persists, your Hugging Face cache might be corrupted. Try clearing it (usually at ~/.cache/huggingface/transformers/).
2025-05-27 12:16:34,914 INFO SentencePiece library found.
2025-05-27 12:16:34,915 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:16:34,915 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:16:34,915 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:16:41,347 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:16:41,348 INFO Attempting to load primary NLI model (microsoft/deberta-base-mnli)...
2025-05-27 12:16:42,376 ERROR Failed to initialize primary NLI LLM interface (microsoft/deberta-base-mnli): Can't load config for 'microsoft/deberta-base-mnli'. Make sure that:

- 'microsoft/deberta-base-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-base-mnli' is the correct path to a directory containing a config.json file


2025-05-27 12:16:42,378 ERROR If 'bert-base-uncased' loaded but this DeBERTa model failed, the issue might be specific to DeBERTa loading in your environment.
2025-05-27 12:16:42,379 ERROR Consider upgrading the 'transformers' library if possible, or try clearing the Hugging Face cache.
2025-05-27 12:17:55,905 INFO SentencePiece library found.
2025-05-27 12:17:55,906 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:17:55,907 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:17:55,907 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:18:02,389 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:18:02,389 INFO Attempting to load primary NLI model (microsoft/deberta-base-mnli)...
2025-05-27 12:18:03,415 ERROR Failed to initialize primary NLI LLM interface (microsoft/deberta-base-mnli): Can't load config for 'microsoft/deberta-base-mnli'. Make sure that:

- 'microsoft/deberta-base-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-base-mnli' is the correct path to a directory containing a config.json file


2025-05-27 12:18:03,416 ERROR If 'bert-base-uncased' loaded but this DeBERTa model failed, the issue might be specific to DeBERTa loading in your environment.
2025-05-27 12:18:03,417 ERROR Consider upgrading the 'transformers' library if possible, or try clearing the Hugging Face cache.
2025-05-27 12:22:36,128 INFO SentencePiece library found.
2025-05-27 12:22:36,129 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:22:36,129 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:22:36,130 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:22:42,727 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:22:42,728 INFO Attempting to load primary NLI model (textattack/bert-base-uncased-MNLI)...
2025-05-27 12:23:38,302 INFO Using LLM: HFNLIRelevanceLLM with model N/A
2025-05-27 12:23:38,302 INFO MeaningAwareSelection initialized successfully.
2025-05-27 12:23:38,303 INFO 
--- Testing filter for stock: AAPL ---
2025-05-27 12:23:38,304 INFO Original texts:
2025-05-27 12:23:38,304 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:23:38,305 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:23:38,305 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:23:38,306 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:23:38,306 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-27 12:23:38,307 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:23:38,307 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:23:38,308 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-27 12:23:38,310 INFO 
--- Filter Results ---
2025-05-27 12:23:38,311 INFO Found 0 RELEVANT texts for AAPL:
2025-05-27 12:23:38,311 INFO   None
2025-05-27 12:23:38,312 INFO 
Found 8 IRRELEVANT texts for AAPL:
2025-05-27 12:23:38,312 INFO   IR1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:23:38,313 INFO   IR2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:23:38,313 INFO   IR3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:23:38,314 INFO   IR4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:23:38,314 INFO   IR5. A local bakery won the award for the best croissant in town.
2025-05-27 12:23:38,315 INFO   IR6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:23:38,315 INFO   IR7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:23:38,316 INFO   IR8. The global chip shortage continues to affect production across industries.
2025-05-27 12:23:38,316 INFO 
--- Test Complete ---
2025-05-27 12:28:42,494 INFO SentencePiece library found.
2025-05-27 12:28:42,494 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:28:42,495 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:28:42,495 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:28:48,983 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:28:48,985 INFO Attempting to load primary NLI model (textattack/bert-base-uncased-MNLI)...
2025-05-27 12:28:57,580 INFO Using LLM: HFNLIRelevanceLLM with model textattack/bert-base-uncased-MNLI
2025-05-27 12:28:57,581 INFO MeaningAwareSelection initialized successfully.
2025-05-27 12:28:57,581 INFO 
--- Testing filter for stock: AAPL ---
2025-05-27 12:28:57,582 INFO Original texts:
2025-05-27 12:28:57,583 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:28:57,583 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:28:57,584 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:28:57,584 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:28:57,585 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-27 12:28:57,585 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:28:57,585 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:28:57,586 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-27 12:28:58,282 INFO 
--- Filter Results ---
2025-05-27 12:28:58,283 INFO Found 0 RELEVANT texts for AAPL:
2025-05-27 12:28:58,283 INFO   None
2025-05-27 12:28:58,284 INFO 
Found 8 IRRELEVANT texts for AAPL:
2025-05-27 12:28:58,284 INFO   IR1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:28:58,285 INFO   IR2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:28:58,285 INFO   IR3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:28:58,286 INFO   IR4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:28:58,286 INFO   IR5. A local bakery won the award for the best croissant in town.
2025-05-27 12:28:58,287 INFO   IR6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:28:58,287 INFO   IR7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:28:58,288 INFO   IR8. The global chip shortage continues to affect production across industries.
2025-05-27 12:28:58,288 INFO 
--- Test Complete ---
2025-05-27 12:31:28,457 INFO SentencePiece library found.
2025-05-27 12:31:28,458 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:31:28,458 INFO Initializing MeaningAwareSelection for NLI-based testing...
2025-05-27 12:31:28,458 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:31:34,917 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:31:34,917 INFO Attempting to load primary NLI model (textattack/bert-base-uncased-MNLI)...
2025-05-27 12:31:43,124 INFO Using LLM: HFNLIRelevanceLLM with model textattack/bert-base-uncased-MNLI
2025-05-27 12:31:43,124 INFO MeaningAwareSelection initialized successfully.
2025-05-27 12:31:43,125 INFO 
--- Testing filter for stock: AAPL ---
2025-05-27 12:31:43,125 INFO Original texts:
2025-05-27 12:31:43,126 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:31:43,126 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:31:43,127 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:31:43,127 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:31:43,127 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-27 12:31:43,128 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:31:43,128 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:31:43,129 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-27 12:31:43,741 INFO 
--- Filter Results ---
2025-05-27 12:31:43,742 INFO Found 7 RELEVANT texts for AAPL:
2025-05-27 12:31:43,743 INFO   R1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 12:31:43,743 INFO   R2. The weather today is sunny with a high of 75 degrees.
2025-05-27 12:31:43,744 INFO   R3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 12:31:43,745 INFO   R4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 12:31:43,745 INFO   R5. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 12:31:43,746 INFO   R6. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 12:31:43,746 INFO   R7. The global chip shortage continues to affect production across industries.
2025-05-27 12:31:43,747 INFO 
Found 1 IRRELEVANT texts for AAPL:
2025-05-27 12:31:43,747 INFO   IR1. A local bakery won the award for the best croissant in town.
2025-05-27 12:31:43,747 INFO 
--- Test Complete ---
2025-05-27 12:40:43,676 INFO SentencePiece library found.
2025-05-27 12:40:43,677 WARNING DEFAULT data_path not found in config, cannot create dummy preprocessed path.
2025-05-27 12:40:43,678 INFO Initializing MeaningAwareSelection for NLI-based testing with ALBERT...
2025-05-27 12:40:43,678 INFO Attempting to load a fallback model (bert-base-uncased) for diagnostics...
2025-05-27 12:40:50,243 INFO Fallback model (bert-base-uncased) config and tokenizer loaded successfully (or from cache).
2025-05-27 12:40:50,244 INFO Attempting to load primary NLI model (albert-base-v2-mnli)...
2025-05-27 12:40:51,238 ERROR Failed to initialize primary NLI LLM interface (albert-base-v2-mnli): Can't load config for 'albert-base-v2-mnli'. Make sure that:

- 'albert-base-v2-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'albert-base-v2-mnli' is the correct path to a directory containing a config.json file


2025-05-27 12:40:51,239 ERROR If this ALBERT NLI model fails, consider upgrading 'transformers' in a new environment.
2025-05-27 18:13:34,443 INFO SentencePiece library found.
2025-05-27 18:13:34,444 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:13:34,445 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:13:34,446 INFO Attempting to load FinBERT model (ProsusAI/finbert)...
2025-05-27 18:13:36,691 ERROR Failed to initialize FinBERT LLM interface (ProsusAI/finbert): Can't load config for 'ProsusAI/finbert'. Make sure that:

- 'ProsusAI/finbert' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'ProsusAI/finbert' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 33, in test_meaning_aware_filter_finbert
    llm_interface = create_llm(finbert_llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 380, in create_llm
    device=config.get("device")
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 393, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'ProsusAI/finbert'. Make sure that:

- 'ProsusAI/finbert' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'ProsusAI/finbert' is the correct path to a directory containing a config.json file


2025-05-27 18:13:36,695 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:17:05,545 INFO SentencePiece library found.
2025-05-27 18:17:05,546 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:17:05,547 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:17:05,547 INFO Attempting to load DIAGNOSTIC model (bert-base-uncased)...
2025-05-27 18:17:12,302 ERROR Failed to initialize FinBERT LLM interface (bert-base-uncased): 'BertTokenizer' object has no attribute 'name_or_path'
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 40, in test_meaning_aware_filter_finbert
    logger.info(f"Using LLM: {type(llm_interface).__name__} with model {getattr(llm_interface, 'tokenizer', {}).name_or_path}")
AttributeError: 'BertTokenizer' object has no attribute 'name_or_path'
2025-05-27 18:17:12,303 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:19:29,146 INFO SentencePiece library found.
2025-05-27 18:19:29,147 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:19:29,148 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:19:29,148 INFO Attempting to load FinBERT model (ProsusAI/finbert)...
2025-05-27 18:19:31,246 ERROR Failed to initialize FinBERT LLM interface (ProsusAI/finbert): Can't load config for 'ProsusAI/finbert'. Make sure that:

- 'ProsusAI/finbert' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'ProsusAI/finbert' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 35, in test_meaning_aware_filter_finbert
    llm_interface = create_llm(finbert_llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 380, in create_llm
    device=config.get("device")
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 393, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'ProsusAI/finbert'. Make sure that:

- 'ProsusAI/finbert' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'ProsusAI/finbert' is the correct path to a directory containing a config.json file


2025-05-27 18:19:31,250 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:20:22,045 INFO SentencePiece library found.
2025-05-27 18:20:22,046 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:20:22,047 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:20:22,047 INFO Attempting to load FinBERT model (bert-base-uncased)...
2025-05-27 18:20:28,388 ERROR Failed to initialize FinBERT LLM interface (bert-base-uncased): 'HFSequenceClassifierLLM_legacy' object has no attribute 'model_name'
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 40, in test_meaning_aware_filter_finbert
    logger.info(f"Using LLM: {type(llm_interface).__name__} with model {llm_interface.model_name}")
AttributeError: 'HFSequenceClassifierLLM_legacy' object has no attribute 'model_name'
2025-05-27 18:20:28,389 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:25:10,683 INFO SentencePiece library found.
2025-05-27 18:25:10,684 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:25:10,684 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:25:10,685 INFO Attempting to load FinBERT model (textattack/bert-base-uncased-mnli)...
2025-05-27 18:25:12,809 ERROR Failed to initialize FinBERT LLM interface (textattack/bert-base-uncased-mnli): Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 35, in test_meaning_aware_filter_finbert
    llm_interface = create_llm(finbert_llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 363, in create_llm
    device=config.get("device")
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 147, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file


2025-05-27 18:25:12,813 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:31:21,609 INFO SentencePiece library found.
2025-05-27 18:31:21,610 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:31:21,611 INFO Initializing MeaningAwareSelection for relevance testing with FinBERT...
2025-05-27 18:31:21,611 INFO Attempting to load NLI model (bert-base-uncased)...
2025-05-27 18:31:23,730 ERROR Failed to initialize NLI LLM interface (bert-base-uncased): Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 37, in test_meaning_aware_filter_finbert
    llm_interface = create_llm(nli_llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 363, in create_llm
    device=config.get("device")
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 147, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file


2025-05-27 18:31:23,733 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 18:54:05,119 INFO SentencePiece library found.
2025-05-27 18:54:05,120 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 18:54:05,121 INFO Initializing MeaningAwareSelection for relevance testing with a BERT-style NLI model...
2025-05-27 18:54:05,121 INFO Attempting to load NLI model (bert-base-uncased)...
2025-05-27 18:54:07,263 ERROR Failed to initialize NLI LLM interface (bert-base-uncased): Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 34, in test_meaning_aware_filter_bert_nli
    llm_interface = create_llm(nli_llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 363, in create_llm
    device=config.get("device")
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 147, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'microsoft/deberta-v3-small-mnli'. Make sure that:

- 'microsoft/deberta-v3-small-mnli' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'microsoft/deberta-v3-small-mnli' is the correct path to a directory containing a config.json file


2025-05-27 18:54:07,267 ERROR Ensure the model name is correct and transformers library is installed.
2025-05-27 19:01:43,281 INFO SentencePiece library found.
2025-05-27 19:01:43,282 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 19:01:43,283 INFO Initializing MeaningAwareSelection for relevance testing with a BERT-style NLI model...
2025-05-27 19:01:43,283 INFO Attempting to load NLI model (bert-base-uncased)...
2025-05-27 19:01:49,931 INFO Using LLM: HFNLIRelevanceLLM with model bert-base-uncased
2025-05-27 19:01:49,932 INFO MeaningAwareSelection initialized successfully.
2025-05-27 19:01:49,932 INFO 
--- Testing filter for stock: AAPL with bert-base-uncased (as NLI) ---
2025-05-27 19:01:49,933 INFO Original texts:
2025-05-27 19:01:49,933 INFO 1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 19:01:49,934 INFO 2. The weather today is sunny with a high of 75 degrees.
2025-05-27 19:01:49,935 INFO 3. AAPL stock price surged by 5% after the product announcement.
2025-05-27 19:01:49,935 INFO 4. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 19:01:49,935 INFO 5. A local bakery won the award for the best croissant in town.
2025-05-27 19:01:49,936 INFO 6. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 19:01:49,936 INFO 7. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 19:01:49,937 INFO 8. The global chip shortage continues to affect production across industries.
2025-05-27 19:01:49,937 INFO 9. Apple (AAPL) faces new lawsuits regarding app store policies.
2025-05-27 19:01:49,938 INFO 10. The market remains volatile due to inflation concerns, AAPL unaffected.
2025-05-27 19:01:50,696 INFO 
--- Filter Results (bert-base-uncased as NLI) ---
2025-05-27 19:01:50,697 INFO Model 'bert-base-uncased' used with HFNLIRelevanceLLM. If it's a base model, its NLI head is randomly initialized.
2025-05-27 19:01:50,697 INFO HFNLIRelevanceLLM maps a detected 'entailment' label to 'Relevant'. Check entailment ID log during init.
2025-05-27 19:01:50,698 INFO Found 4 RELEVANT texts for AAPL:
2025-05-27 19:01:50,698 INFO   R1. Apple's new iPhone 15 is expected to boost sales significantly this quarter.
2025-05-27 19:01:50,699 INFO   R2. The weather today is sunny with a high of 75 degrees.
2025-05-27 19:01:50,699 INFO   R3. A local bakery won the award for the best croissant in town.
2025-05-27 19:01:50,700 INFO   R4. Rumors suggest Apple might be developing a new VR headset.
2025-05-27 19:01:50,700 INFO 
Found 6 IRRELEVANT texts for AAPL:
2025-05-27 19:01:50,701 INFO   IR1. AAPL stock price surged by 5% after the product announcement.
2025-05-27 19:01:50,701 INFO   IR2. Analysts are bullish on Apple (AAPL) following strong earnings reports.
2025-05-27 19:01:50,702 INFO   IR3. SEC is investigating trading activities related to several tech stocks, not including Apple.
2025-05-27 19:01:50,702 INFO   IR4. The global chip shortage continues to affect production across industries.
2025-05-27 19:01:50,703 INFO   IR5. Apple (AAPL) faces new lawsuits regarding app store policies.
2025-05-27 19:01:50,703 INFO   IR6. The market remains volatile due to inflation concerns, AAPL unaffected.
2025-05-27 19:01:50,704 INFO 
--- {nli_model_name} (as NLI) Test Complete ---
2025-05-27 20:38:32,227 INFO SentencePiece library found.
2025-05-27 20:38:32,229 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 20:38:32,229 INFO Initializing MeaningAwareSelection for relevance testing with OpenFinAL/GPT2_FINGPT_QA...
2025-05-27 20:38:38,563 ERROR Failed to initialize LLM interface (OpenFinAL/GPT2_FINGPT_QA): Model name 'OpenFinAL/GPT2_FINGPT_QA' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'OpenFinAL/GPT2_FINGPT_QA' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 384, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 243, in __init__
    self.tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 911, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 1014, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'OpenFinAL/GPT2_FINGPT_QA' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'OpenFinAL/GPT2_FINGPT_QA' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
2025-05-27 20:43:13,705 INFO SentencePiece library found.
2025-05-27 20:43:13,706 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 20:43:13,707 INFO Initializing MeaningAwareSelection for relevance testing with OpenFinAL/GPT2_FINGPT_QA...
2025-05-27 20:43:23,270 ERROR Failed to initialize LLM interface (OpenFinAL/GPT2_FINGPT_QA): Can't load config for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a config.json file

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 395, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 251, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path).to(self.device)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 587, in from_pretrained
    **kwargs,
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 201, in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a config.json file


2025-05-27 20:46:38,132 INFO SentencePiece library found.
2025-05-27 20:46:38,133 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 20:46:38,133 INFO Initializing MeaningAwareSelection for relevance testing with OpenFinAL/GPT2_FINGPT_QA...
2025-05-27 20:46:45,606 ERROR Failed to initialize LLM interface (OpenFinAL/GPT2_FINGPT_QA): Can't load weights for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 254, in __init__
    config = GPT2Config.from_pretrained(model_name_or_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 201, in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a config.json file



During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 638, in from_pretrained
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 413, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 268, in __init__
    raise e_fallback # Re-raise the fallback error
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 265, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=base_config).to(self.device)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 645, in from_pretrained
    raise EnvironmentError(msg)
OSError: Can't load weights for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.


2025-05-27 20:52:43,774 INFO SentencePiece library found.
2025-05-27 20:52:43,775 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 20:52:43,776 INFO Initializing MeaningAwareSelection for relevance testing with OpenFinAL/GPT2_FINGPT_QA...
2025-05-27 20:52:49,860 ERROR Failed to initialize LLM interface (OpenFinAL/GPT2_FINGPT_QA): Can't load weights for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 254, in __init__
    config = GPT2Config.from_pretrained(model_name_or_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 201, in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a config.json file



During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 638, in from_pretrained
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 413, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 268, in __init__
    raise e_fallback # Re-raise the fallback error
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 265, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=base_config).to(self.device)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 645, in from_pretrained
    raise EnvironmentError(msg)
OSError: Can't load weights for 'OpenFinAL/GPT2_FINGPT_QA'. Make sure that:

- 'OpenFinAL/GPT2_FINGPT_QA' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'OpenFinAL/GPT2_FINGPT_QA' is the correct path to a directory containing a file named one of pytorch_model.bin, tf_model.h5, model.ckpt.


2025-05-27 21:07:06,813 INFO SentencePiece library found.
2025-05-27 21:07:06,814 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-27 21:07:06,815 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-27 21:07:09,094 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA): Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA or `from_tf` set to False
Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 256, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config).to(self.device)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 608, in from_pretrained
    pretrained_model_name_or_path,
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA or `from_tf` set to False

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 413, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 268, in __init__
    raise e_fallback # Re-raise the fallback error
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 265, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=base_config).to(self.device)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 608, in from_pretrained
    pretrained_model_name_or_path,
OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA or `from_tf` set to False
2025-05-28 10:52:09,719 INFO SentencePiece library found.
2025-05-28 10:52:09,721 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 10:52:09,721 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 10:52:12,792 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA
2025-05-28 10:52:12,793 INFO MeaningAwareSelection initialized successfully.
2025-05-28 10:52:12,793 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA ---
2025-05-28 10:52:12,794 INFO Original texts:
2025-05-28 10:52:12,794 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 10:52:12,795 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 10:52:12,795 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 10:52:12,796 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 10:52:12,823 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA) ---
2025-05-28 10:52:12,824 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 10:52:12,825 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 10:52:12,825 INFO   None
2025-05-28 10:52:12,826 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 10:52:12,826 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 10:52:12,827 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 10:52:12,827 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 10:52:12,828 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 10:52:12,828 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA Test Complete ---
2025-05-28 11:06:10,822 INFO SentencePiece library found.
2025-05-28 11:06:10,823 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:06:10,824 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:06:10,906 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA): Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel or AutoModelForCausalLM.
Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 260, in __init__
    ignore_mismatched_sizes=True  # Allow loading even if some head sizes mismatch (e.g. for fine-tuning)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\modeling_utils.py", line 655, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
TypeError: __init__() got an unexpected keyword argument 'ignore_mismatched_sizes'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 273, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
NameError: name 'AutoModelForCausalLM' is not defined

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 418, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 281, in __init__
    raise ValueError(f"Could not load model from {model_name_or_path} using GPT2LMHeadModel or AutoModelForCausalLM.") from e_auto
ValueError: Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel or AutoModelForCausalLM.
2025-05-28 11:07:05,809 INFO SentencePiece library found.
2025-05-28 11:07:05,809 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:07:05,811 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:07:05,890 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA): 'GenerativeRelevanceLLM' object has no attribute 'model'
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 419, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 284, in __init__
    self.model.eval()
AttributeError: 'GenerativeRelevanceLLM' object has no attribute 'model'
2025-05-28 11:18:34,434 INFO SentencePiece library found.
2025-05-28 11:18:34,435 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:18:34,436 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:18:37,457 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA
2025-05-28 11:18:37,458 INFO MeaningAwareSelection initialized successfully.
2025-05-28 11:18:37,458 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA ---
2025-05-28 11:18:37,459 INFO Original texts:
2025-05-28 11:18:37,460 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:18:37,460 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 11:18:37,461 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:18:37,462 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:18:37,469 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA) ---
2025-05-28 11:18:37,470 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 11:18:37,471 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 11:18:37,472 INFO   None
2025-05-28 11:18:37,472 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 11:18:37,473 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:18:37,474 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 11:18:37,474 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:18:37,475 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:18:37,476 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA Test Complete ---
2025-05-28 11:22:14,788 INFO SentencePiece library found.
2025-05-28 11:22:14,789 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:22:14,789 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:22:17,734 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA
2025-05-28 11:22:17,735 INFO MeaningAwareSelection initialized successfully.
2025-05-28 11:22:17,736 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA ---
2025-05-28 11:22:17,736 INFO Original texts:
2025-05-28 11:22:17,737 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:22:17,737 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 11:22:17,738 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:22:17,738 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:22:17,744 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA) ---
2025-05-28 11:22:17,745 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 11:22:17,746 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 11:22:17,746 INFO   None
2025-05-28 11:22:17,746 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 11:22:17,747 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:22:17,748 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 11:22:17,748 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:22:17,749 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:22:17,749 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA Test Complete ---
2025-05-28 11:30:20,896 INFO SentencePiece library found.
2025-05-28 11:30:20,897 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:30:20,898 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter...
2025-05-28 11:30:20,903 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter): Model name 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 404, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 250, in __init__
    self.tokenizer = GPT2Tokenizer.from_pretrained(actual_tokenizer_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 911, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 1014, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt-2-finance-adapter' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
2025-05-28 11:46:44,126 INFO SentencePiece library found.
2025-05-28 11:46:44,127 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:46:44,128 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:46:47,382 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA
2025-05-28 11:46:47,382 INFO MeaningAwareSelection initialized successfully.
2025-05-28 11:46:47,383 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA ---
2025-05-28 11:46:47,384 INFO Original texts:
2025-05-28 11:46:47,384 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:46:47,385 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 11:46:47,386 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:46:47,386 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:46:47,395 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA) ---
2025-05-28 11:46:47,395 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 11:46:47,396 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 11:46:47,397 INFO   None
2025-05-28 11:46:47,397 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 11:46:47,398 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 11:46:47,398 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 11:46:47,399 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 11:46:47,399 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 11:46:47,400 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA Test Complete ---
2025-05-28 11:49:44,554 INFO SentencePiece library found.
2025-05-28 11:49:44,555 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:49:44,556 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:49:47,237 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA): Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel.
Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 262, in __init__
    state_dict = torch.load("pytorch_model.bin", map_location="cpu")
  File "D:\tool\anaconda\envs\py36\lib\site-packages\torch\serialization.py", line 579, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "D:\tool\anaconda\envs\py36\lib\site-packages\torch\serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\torch\serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'pytorch_model.bin'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 409, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 272, in __init__
    raise ValueError(f"Could not load model from {model_name_or_path} using GPT2LMHeadModel.") from e
ValueError: Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel.
2025-05-28 11:53:04,005 INFO SentencePiece library found.
2025-05-28 11:53:04,006 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 11:53:04,007 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA...
2025-05-28 11:53:06,983 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA): Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel.
Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 265, in __init__
    self.model.load_state_dict(state_dict)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\torch\nn\modules\module.py", line 1224, in load_state_dict
    self.__class__.__name__, "\n\t".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel:
	Missing key(s) in state_dict: "transformer.h.0.attn.bias", "transformer.h.0.attn.masked_bias", "transformer.h.1.attn.bias", "transformer.h.1.attn.masked_bias", "transformer.h.2.attn.bias", "transformer.h.2.attn.masked_bias", "transformer.h.3.attn.bias", "transformer.h.3.attn.masked_bias", "transformer.h.4.attn.bias", "transformer.h.4.attn.masked_bias", "transformer.h.5.attn.bias", "transformer.h.5.attn.masked_bias", "transformer.h.6.attn.bias", "transformer.h.6.attn.masked_bias", "transformer.h.7.attn.bias", "transformer.h.7.attn.masked_bias", "transformer.h.8.attn.bias", "transformer.h.8.attn.masked_bias", "transformer.h.9.attn.bias", "transformer.h.9.attn.masked_bias", "transformer.h.10.attn.bias", "transformer.h.10.attn.masked_bias", "transformer.h.11.attn.bias", "transformer.h.11.attn.masked_bias". 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 412, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 275, in __init__
    raise ValueError(f"Could not load model from {model_name_or_path} using GPT2LMHeadModel.") from e
ValueError: Could not load model from D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\GPT2_FINGPT_QA using GPT2LMHeadModel.
2025-05-28 12:32:20,977 INFO SentencePiece library found.
2025-05-28 12:32:20,978 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:32:20,978 INFO Initializing MeaningAwareSelection for relevance testing with Hugging Face model: jakobwes/finance-gpt2...
2025-05-28 12:32:28,379 ERROR Failed to initialize LLM interface (jakobwes/finance-gpt2): Model name 'jakobwes/finance-gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'jakobwes/finance-gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 408, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 250, in __init__
    self.tokenizer = GPT2Tokenizer.from_pretrained(actual_tokenizer_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 911, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 1014, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'jakobwes/finance-gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'jakobwes/finance-gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
2025-05-28 12:40:43,910 INFO SentencePiece library found.
2025-05-28 12:40:43,911 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:40:43,912 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:40:47,063 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 12:40:47,064 INFO MeaningAwareSelection initialized successfully.
2025-05-28 12:40:47,065 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 12:40:47,065 INFO Original texts:
2025-05-28 12:40:47,066 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:40:47,066 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 12:40:47,067 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:40:47,067 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:40:47,077 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 12:40:47,077 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 12:40:47,078 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 12:40:47,079 INFO   None
2025-05-28 12:40:47,079 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 12:40:47,080 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:40:47,081 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 12:40:47,081 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:40:47,082 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:40:47,082 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 12:42:55,287 INFO SentencePiece library found.
2025-05-28 12:42:55,288 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:42:55,288 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:42:58,281 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2): can't set attribute
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 414, in create_llm
    temperature=config.get("temperature", 0.1)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 277, in __init__
    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
AttributeError: can't set attribute
2025-05-28 12:44:25,428 INFO SentencePiece library found.
2025-05-28 12:44:25,431 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:44:25,431 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:44:28,347 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 12:44:28,348 INFO MeaningAwareSelection initialized successfully.
2025-05-28 12:44:28,348 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 12:44:28,349 INFO Original texts:
2025-05-28 12:44:28,350 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:44:28,350 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 12:44:28,351 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:44:28,351 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:44:28,358 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 12:44:28,358 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 12:44:28,359 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 12:44:28,360 INFO   None
2025-05-28 12:44:28,360 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 12:44:28,361 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:44:28,361 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 12:44:28,362 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:44:28,363 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:44:28,363 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 12:46:20,805 INFO SentencePiece library found.
2025-05-28 12:46:20,806 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:46:20,806 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:46:23,738 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 12:46:23,738 INFO MeaningAwareSelection initialized successfully.
2025-05-28 12:46:23,739 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 12:46:23,740 INFO Original texts:
2025-05-28 12:46:23,740 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:46:23,741 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 12:46:23,741 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:46:23,742 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:46:23,748 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 12:46:23,748 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 12:46:23,749 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 12:46:23,750 INFO   None
2025-05-28 12:46:23,750 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 12:46:23,751 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:46:23,751 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 12:46:23,752 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:46:23,752 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:46:23,753 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 12:48:02,111 INFO SentencePiece library found.
2025-05-28 12:48:02,112 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:48:02,112 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:48:05,003 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 12:48:05,004 INFO MeaningAwareSelection initialized successfully.
2025-05-28 12:48:05,004 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 12:48:05,005 INFO Original texts:
2025-05-28 12:48:05,005 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:48:05,006 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 12:48:05,006 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:48:05,007 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:48:05,012 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 12:48:05,012 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 12:48:05,013 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 12:48:05,013 INFO   None
2025-05-28 12:48:05,014 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 12:48:05,014 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:48:05,015 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 12:48:05,015 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:48:05,016 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:48:05,016 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 12:50:37,393 INFO SentencePiece library found.
2025-05-28 12:50:37,395 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:50:37,395 INFO Initializing MeaningAwareSelection for relevance testing with local model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 12:50:40,394 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 12:50:40,395 INFO MeaningAwareSelection initialized successfully.
2025-05-28 12:50:40,395 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 12:50:40,396 INFO Original texts:
2025-05-28 12:50:40,396 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:50:40,397 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 12:50:40,397 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:50:40,398 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:50:40,407 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 12:50:40,408 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 12:50:40,409 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 12:50:40,410 INFO   None
2025-05-28 12:50:40,410 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 12:50:40,411 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 12:50:40,411 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 12:50:40,412 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 12:50:40,413 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 12:50:40,413 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 12:53:01,710 INFO SentencePiece library found.
2025-05-28 12:53:01,711 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:53:01,712 INFO Initializing MeaningAwareSelection for relevance testing with Hugging Face model: openai-community/gpt2...
2025-05-28 12:53:08,262 ERROR Failed to initialize LLM interface (openai-community/gpt2): Model name 'openai-community/gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'openai-community/gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 428, in create_llm
    do_sample=config.get("do_sample", True)    # Explicitly enable sampling by default
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 250, in __init__
    self.tokenizer = GPT2Tokenizer.from_pretrained(actual_tokenizer_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 911, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 1014, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'openai-community/gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'openai-community/gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
2025-05-28 12:55:19,199 INFO SentencePiece library found.
2025-05-28 12:55:19,200 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 12:55:19,200 INFO Initializing MeaningAwareSelection for relevance testing with Hugging Face model: openai-community/gpt2...
2025-05-28 12:56:04,777 ERROR Failed to initialize LLM interface (openai-community/gpt2): Model name 'openai-community/gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'openai-community/gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
Traceback (most recent call last):
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 243, in get_config_dict
    raise EnvironmentError
OSError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 253, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(actual_tokenizer_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_auto.py", line 195, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_auto.py", line 196, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\configuration_utils.py", line 252, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'openai-community/gpt2'. Make sure that:

- 'openai-community/gpt2' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'openai-community/gpt2' is the correct path to a directory containing a config.json file



During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 440, in create_llm
    do_sample=config.get("do_sample", True)    # Explicitly enable sampling by default
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 262, in __init__
    raise e_gpt2_tok # Re-raise the gpt2 tokenizer error if fallback also fails
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 258, in __init__
    self.tokenizer = GPT2Tokenizer.from_pretrained(actual_tokenizer_path)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 911, in from_pretrained
    return cls._from_pretrained(*inputs, **kwargs)
  File "D:\tool\anaconda\envs\py36\lib\site-packages\transformers\tokenization_utils.py", line 1014, in _from_pretrained
    list(cls.vocab_files_names.values()),
OSError: Model name 'openai-community/gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'openai-community/gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.
2025-05-28 13:07:40,670 INFO SentencePiece library found.
2025-05-28 13:07:40,671 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:07:40,672 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2...
2025-05-28 13:07:43,923 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2
2025-05-28 13:07:43,924 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:07:43,924 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 ---
2025-05-28 13:07:43,925 INFO Original texts:
2025-05-28 13:07:43,925 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:07:43,926 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:07:43,926 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:07:43,927 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:07:43,954 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2) ---
2025-05-28 13:07:43,955 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:07:43,955 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:07:43,956 INFO   None
2025-05-28 13:07:43,956 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:07:43,959 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:07:43,960 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:07:43,960 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:07:43,961 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:07:43,962 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 Test Complete ---
2025-05-28 13:09:39,051 INFO SentencePiece library found.
2025-05-28 13:09:39,052 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:09:39,053 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2...
2025-05-28 13:09:42,006 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2
2025-05-28 13:09:42,006 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:09:42,007 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 ---
2025-05-28 13:09:42,007 INFO Original texts:
2025-05-28 13:09:42,008 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:09:42,008 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:09:42,008 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:09:42,009 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:09:42,019 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2) ---
2025-05-28 13:09:42,019 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:09:42,020 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:09:42,020 INFO   None
2025-05-28 13:09:42,021 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:09:42,022 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:09:42,022 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:09:42,023 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:09:42,023 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:09:42,024 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 Test Complete ---
2025-05-28 13:11:15,211 INFO SentencePiece library found.
2025-05-28 13:11:15,211 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:11:15,212 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2...
2025-05-28 13:11:18,151 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2
2025-05-28 13:11:18,152 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:11:18,154 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 ---
2025-05-28 13:11:18,155 INFO Original texts:
2025-05-28 13:11:18,155 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:11:18,156 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:11:18,156 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:11:18,157 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:11:18,167 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2) ---
2025-05-28 13:11:18,168 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:11:18,169 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:11:18,169 INFO   None
2025-05-28 13:11:18,170 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:11:18,170 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:11:18,171 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:11:18,172 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:11:18,172 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:11:18,173 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\gpt2 Test Complete ---
2025-05-28 13:13:46,675 INFO SentencePiece library found.
2025-05-28 13:13:46,676 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:13:46,677 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 13:13:49,747 ERROR Failed to initialize LLM interface (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2): 'GenerativeRelevanceLLM' object has no attribute 'tokenizer'
Traceback (most recent call last):
  File "src/test_filter_efficiency.py", line 123, in test_meaning_aware_filter_fingpt_gpt2_qa
    llm_interface = create_llm(llm_config)
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 445, in create_llm
    do_sample=config.get("do_sample", True)    # Explicitly enable sampling by default
  File "D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\src\LLMInterface.py", line 292, in __init__
    if self.tokenizer.pad_token is None:
AttributeError: 'GenerativeRelevanceLLM' object has no attribute 'tokenizer'
2025-05-28 13:16:40,496 INFO SentencePiece library found.
2025-05-28 13:16:40,497 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:16:40,497 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 13:16:43,465 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 13:16:43,466 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:16:43,467 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 13:16:43,467 INFO Original texts:
2025-05-28 13:16:43,468 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:16:43,468 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:16:43,469 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:16:43,470 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:16:43,480 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 13:16:43,481 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:16:43,481 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:16:43,482 INFO   None
2025-05-28 13:16:43,482 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:16:43,483 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:16:43,483 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:16:43,484 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:16:43,484 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:16:43,485 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 13:18:39,934 INFO SentencePiece library found.
2025-05-28 13:18:39,935 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:18:39,936 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 13:18:42,875 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 13:18:42,876 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:18:42,876 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 13:18:42,877 INFO Original texts:
2025-05-28 13:18:42,877 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:18:42,878 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:18:42,878 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:18:42,879 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:18:42,891 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 13:18:42,892 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:18:42,893 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:18:42,893 INFO   None
2025-05-28 13:18:42,894 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:18:42,894 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:18:42,895 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:18:42,896 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:18:42,896 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:18:42,897 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 13:48:15,589 INFO SentencePiece library found.
2025-05-28 13:48:15,590 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:48:15,590 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 13:48:18,634 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 13:48:18,635 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:48:18,635 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 13:48:18,636 INFO Original texts:
2025-05-28 13:48:18,637 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:48:18,637 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:48:18,638 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:48:18,638 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:48:18,657 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 13:48:18,657 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:48:18,658 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:48:18,659 INFO   None
2025-05-28 13:48:18,659 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:48:18,660 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:48:18,661 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:48:18,661 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:48:18,662 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:48:18,662 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 13:51:21,756 INFO SentencePiece library found.
2025-05-28 13:51:21,758 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 13:51:21,758 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 13:51:24,719 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 13:51:24,719 INFO MeaningAwareSelection initialized successfully.
2025-05-28 13:51:24,720 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 13:51:24,720 INFO Original texts:
2025-05-28 13:51:24,721 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:51:24,721 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 13:51:24,722 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:51:24,722 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:51:24,741 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 13:51:24,742 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 13:51:24,743 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 13:51:24,743 INFO   None
2025-05-28 13:51:24,744 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 13:51:24,745 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 13:51:24,745 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 13:51:24,746 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 13:51:24,746 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 13:51:24,747 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-28 14:47:30,816 INFO SentencePiece library found.
2025-05-28 14:47:30,818 INFO Ensured dummy preprocessed path exists: .\preprocessed
2025-05-28 14:47:30,818 INFO Initializing MeaningAwareSelection for relevance testing with local Hugging Face model: D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2...
2025-05-28 14:47:34,204 INFO Using LLM: GenerativeRelevanceLLM with model D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2
2025-05-28 14:47:34,205 INFO MeaningAwareSelection initialized successfully.
2025-05-28 14:47:34,205 INFO 
--- Testing filter for stock: MSFT with D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 ---
2025-05-28 14:47:34,206 INFO Original texts:
2025-05-28 14:47:34,207 INFO 1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 14:47:34,207 INFO 2. The new Xbox series is selling out worldwide.
2025-05-28 14:47:34,207 INFO 3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 14:47:34,208 INFO 4. MSFT stock is up 3% in pre-market trading.
2025-05-28 14:47:34,248 INFO 
--- Filter Results (D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2) ---
2025-05-28 14:47:34,249 INFO Model 'D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2' used with GenerativeRelevanceLLM. Output depends on prompting.
2025-05-28 14:47:34,250 INFO Found 0 RELEVANT texts for MSFT:
2025-05-28 14:47:34,250 INFO   None
2025-05-28 14:47:34,251 INFO 
Found 4 IRRELEVANT texts for MSFT:
2025-05-28 14:47:34,252 INFO   IR1. Microsoft reported strong earnings this quarter, beating analyst expectations.
2025-05-28 14:47:34,252 INFO   IR2. The new Xbox series is selling out worldwide.
2025-05-28 14:47:34,253 INFO   IR3. A new cafe opened downtown, unrelated to any tech company.
2025-05-28 14:47:34,253 INFO   IR4. MSFT stock is up 3% in pre-market trading.
2025-05-28 14:47:34,254 INFO 
--- D:\FinalYear\KLTN\PEN\PEN-main\PEN-main\LLM_Model\finance-gpt2 Test Complete ---
2025-05-29 12:21:05,689 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 12:21:05,689 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 13:01:18,540 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 13:01:18,540 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 16:59:15,002 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 16:59:15,002 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 17:14:10,806 INFO ASSEMBLE: word table #replacement: 19482
2025-05-29 17:14:10,806 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 10:30:47,751 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 10:30:47,751 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 10:41:39,744 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 10:41:39,744 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 11:15:16,693 INFO Epoch: loss: 2.569868, acc: 0.514113
2025-05-30 11:15:16,693 INFO Epoch: loss: 2.569868, acc: 0.514113
2025-05-30 11:48:57,899 INFO Epoch: loss: 1.137355, acc: 0.508871
2025-05-30 11:48:57,899 INFO Epoch: loss: 1.137355, acc: 0.508871
2025-05-30 17:07:43,597 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 17:07:43,597 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 17:10:14,723 INFO ASSEMBLE: word table #replacement: 19482
2025-05-30 17:10:14,723 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 00:46:06,952 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 00:46:06,952 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 01:04:16,919 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 01:04:16,919 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 14:16:03,747 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 14:16:03,747 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 14:23:13,808 INFO 	Eval, eval loss: 6.723052978515625, acc: 0.541159
2025-06-01 14:23:13,808 INFO 	Eval, eval loss: 6.723052978515625, acc: 0.541159
2025-06-01 15:44:14,558 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 15:44:14,558 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 15:51:32,642 INFO 	Eval, eval loss: 4.440765857696533, acc: 0.542683
2025-06-01 15:51:32,642 INFO 	Eval, eval loss: 4.440765857696533, acc: 0.542683
2025-06-01 15:54:02,641 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 15:54:02,641 INFO ASSEMBLE: word table #replacement: 19482
2025-06-01 16:01:20,077 INFO 	Eval, eval loss: 3.282569408416748, acc: 0.524390
2025-06-01 16:01:20,077 INFO 	Eval, eval loss: 3.282569408416748, acc: 0.524390
2025-06-03 11:07:13,139 INFO Starting noise-aware loss test...
2025-06-03 11:07:13,139 INFO Starting noise-aware loss test...
2025-06-03 11:07:13,141 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 11:07:13,141 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 11:07:13,153 INFO Start graph assembling...
2025-06-03 11:07:13,153 INFO Start graph assembling...
2025-06-03 11:07:16,082 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:07:16,082 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:07:16,083 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:07:16,083 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:07:34,644 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 11:07:34,644 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 11:07:36,221 INFO Word table init: done!
2025-06-03 11:07:36,221 INFO Word table init: done!
2025-06-03 11:07:36,907 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 11:07:36,907 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 11:07:36,907 INFO Starting Epoch 1/10...
2025-06-03 11:07:36,907 INFO Starting Epoch 1/10...
2025-06-03 11:08:13,732 INFO --- Epoch 1, Batch 81 (Overall batch 1) ---
2025-06-03 11:08:13,732 INFO --- Epoch 1, Batch 81 (Overall batch 1) ---
2025-06-03 11:08:13,733 INFO Total Loss: 3.243438959121704
2025-06-03 11:08:13,733 INFO Total Loss: 3.243438959121704
2025-06-03 11:08:13,733 INFO Causal Consistency Loss: 0.00481808464974165
2025-06-03 11:08:13,733 INFO Causal Consistency Loss: 0.00481808464974165
2025-06-03 11:08:13,734 INFO Accuracy: 0.6875
2025-06-03 11:08:13,734 INFO Accuracy: 0.6875
2025-06-03 11:08:13,734 INFO MCC: 0.4251
2025-06-03 11:08:13,734 INFO MCC: 0.4251
2025-06-03 11:08:13,736 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:08:13,736 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:08:13,737 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:08:13,737 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:08:13,738 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05569365 0.04450484 0.044412   0.04398391 0.05561339]
2025-06-03 11:08:13,738 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05569365 0.04450484 0.044412   0.04398391 0.05561339]
2025-06-03 11:08:41,382 INFO --- Epoch 1, Batch 82 (Overall batch 2) ---
2025-06-03 11:08:41,382 INFO --- Epoch 1, Batch 82 (Overall batch 2) ---
2025-06-03 11:08:41,383 INFO Total Loss: 3.209378242492676
2025-06-03 11:08:41,383 INFO Total Loss: 3.209378242492676
2025-06-03 11:08:41,385 INFO Causal Consistency Loss: 0.004192798864096403
2025-06-03 11:08:41,385 INFO Causal Consistency Loss: 0.004192798864096403
2025-06-03 11:08:41,385 INFO Accuracy: 0.5938
2025-06-03 11:08:41,385 INFO Accuracy: 0.5938
2025-06-03 11:08:41,386 INFO MCC: 0.2102
2025-06-03 11:08:41,386 INFO MCC: 0.2102
2025-06-03 11:08:41,386 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4836951 0.5163049 0.        0.        0.       ]
2025-06-03 11:08:41,386 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4836951 0.5163049 0.        0.        0.       ]
2025-06-03 11:08:41,387 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4375405  0.56245947 0.         0.         0.        ]
2025-06-03 11:08:41,387 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4375405  0.56245947 0.         0.         0.        ]
2025-06-03 11:08:41,388 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05467336 0.05536044 0.05487698 0.05551702 0.04470043]
2025-06-03 11:08:41,388 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05467336 0.05536044 0.05487698 0.05551702 0.04470043]
2025-06-03 11:08:57,926 INFO --- Epoch 1, Batch 83 (Overall batch 3) ---
2025-06-03 11:08:57,926 INFO --- Epoch 1, Batch 83 (Overall batch 3) ---
2025-06-03 11:08:57,928 INFO Total Loss: 3.0958824157714844
2025-06-03 11:08:57,928 INFO Total Loss: 3.0958824157714844
2025-06-03 11:08:57,928 INFO Causal Consistency Loss: 0.004302547313272953
2025-06-03 11:08:57,928 INFO Causal Consistency Loss: 0.004302547313272953
2025-06-03 11:08:57,929 INFO Accuracy: 0.6250
2025-06-03 11:08:57,929 INFO Accuracy: 0.6250
2025-06-03 11:08:57,929 INFO MCC: 0.2698
2025-06-03 11:08:57,929 INFO MCC: 0.2698
2025-06-03 11:08:57,930 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.531694 0.468306 0.       0.       0.      ]
2025-06-03 11:08:57,930 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.531694 0.468306 0.       0.       0.      ]
2025-06-03 11:08:57,930 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5241058  0.47589418 0.         0.         0.        ]
2025-06-03 11:08:57,930 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5241058  0.47589418 0.         0.         0.        ]
2025-06-03 11:08:57,931 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05506216 0.04538576 0.05544411 0.05676876 0.04417803]
2025-06-03 11:08:57,931 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05506216 0.04538576 0.05544411 0.05676876 0.04417803]
2025-06-03 11:09:21,512 INFO --- Epoch 1, Batch 84 (Overall batch 4) ---
2025-06-03 11:09:21,512 INFO --- Epoch 1, Batch 84 (Overall batch 4) ---
2025-06-03 11:09:21,513 INFO Total Loss: 3.1637229919433594
2025-06-03 11:09:21,513 INFO Total Loss: 3.1637229919433594
2025-06-03 11:09:21,513 INFO Causal Consistency Loss: 0.002873858902603388
2025-06-03 11:09:21,513 INFO Causal Consistency Loss: 0.002873858902603388
2025-06-03 11:09:21,514 INFO Accuracy: 0.6562
2025-06-03 11:09:21,514 INFO Accuracy: 0.6562
2025-06-03 11:09:21,514 INFO MCC: 0.3235
2025-06-03 11:09:21,514 INFO MCC: 0.3235
2025-06-03 11:09:21,515 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4909435 0.5090565 0.        0.        0.       ]
2025-06-03 11:09:21,515 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4909435 0.5090565 0.        0.        0.       ]
2025-06-03 11:09:21,516 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.46443093 0.5355691  0.         0.         0.        ]
2025-06-03 11:09:21,516 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.46443093 0.5355691  0.         0.         0.        ]
2025-06-03 11:09:21,517 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05584428 0.04685094 0.0551783  0.05426108 0.05444679]
2025-06-03 11:09:21,517 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05584428 0.04685094 0.0551783  0.05426108 0.05444679]
2025-06-03 11:09:35,647 INFO --- Epoch 1, Batch 85 (Overall batch 5) ---
2025-06-03 11:09:35,647 INFO --- Epoch 1, Batch 85 (Overall batch 5) ---
2025-06-03 11:09:35,648 INFO Total Loss: 3.1643505096435547
2025-06-03 11:09:35,648 INFO Total Loss: 3.1643505096435547
2025-06-03 11:09:35,648 INFO Causal Consistency Loss: 0.0047083767130970955
2025-06-03 11:09:35,648 INFO Causal Consistency Loss: 0.0047083767130970955
2025-06-03 11:09:35,649 INFO Accuracy: 0.6875
2025-06-03 11:09:35,649 INFO Accuracy: 0.6875
2025-06-03 11:09:35,649 INFO MCC: 0.3317
2025-06-03 11:09:35,649 INFO MCC: 0.3317
2025-06-03 11:09:35,650 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.09983575 0.1730828  0.16744448 0.20865643 0.19540729]
2025-06-03 11:09:35,650 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.09983575 0.1730828  0.16744448 0.20865643 0.19540729]
2025-06-03 11:09:35,651 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.10236272 0.18714137 0.12554461 0.20732814 0.20631796]
2025-06-03 11:09:35,651 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.10236272 0.18714137 0.12554461 0.20732814 0.20631796]
2025-06-03 11:09:35,652 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05597159 0.0332012  0.05772687 0.05514283 0.05457669]
2025-06-03 11:09:35,652 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05597159 0.0332012  0.05772687 0.05514283 0.05457669]
2025-06-03 11:09:51,411 INFO --- Epoch 1, Batch 86 (Overall batch 6) ---
2025-06-03 11:09:51,411 INFO --- Epoch 1, Batch 86 (Overall batch 6) ---
2025-06-03 11:09:51,412 INFO Total Loss: 3.106937885284424
2025-06-03 11:09:51,412 INFO Total Loss: 3.106937885284424
2025-06-03 11:09:51,413 INFO Causal Consistency Loss: 0.005081627052277327
2025-06-03 11:09:51,413 INFO Causal Consistency Loss: 0.005081627052277327
2025-06-03 11:09:51,413 INFO Accuracy: 0.7188
2025-06-03 11:09:51,413 INFO Accuracy: 0.7188
2025-06-03 11:09:51,414 INFO MCC: 0.4454
2025-06-03 11:09:51,414 INFO MCC: 0.4454
2025-06-03 11:09:51,415 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.5440001 0.4559999 0.        0.        0.       ]
2025-06-03 11:09:51,415 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.5440001 0.4559999 0.        0.        0.       ]
2025-06-03 11:09:51,415 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5490715  0.45092848 0.         0.         0.        ]
2025-06-03 11:09:51,415 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5490715  0.45092848 0.         0.         0.        ]
2025-06-03 11:09:51,416 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05549323 0.05570575 0.05569098 0.05484443 0.04446984]
2025-06-03 11:09:51,416 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05549323 0.05570575 0.05569098 0.05484443 0.04446984]
2025-06-03 11:10:05,993 INFO --- Epoch 1, Batch 87 (Overall batch 7) ---
2025-06-03 11:10:05,993 INFO --- Epoch 1, Batch 87 (Overall batch 7) ---
2025-06-03 11:10:05,994 INFO Total Loss: 3.0421783924102783
2025-06-03 11:10:05,994 INFO Total Loss: 3.0421783924102783
2025-06-03 11:10:05,995 INFO Causal Consistency Loss: 0.004625772591680288
2025-06-03 11:10:05,995 INFO Causal Consistency Loss: 0.004625772591680288
2025-06-03 11:10:05,995 INFO Accuracy: 0.5938
2025-06-03 11:10:05,995 INFO Accuracy: 0.5938
2025-06-03 11:10:05,996 INFO MCC: 0.1974
2025-06-03 11:10:05,996 INFO MCC: 0.1974
2025-06-03 11:10:05,996 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.5722989  0.42770118 0.         0.         0.        ]
2025-06-03 11:10:05,996 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.5722989  0.42770118 0.         0.         0.        ]
2025-06-03 11:10:05,997 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.507908   0.49209207 0.         0.         0.        ]
2025-06-03 11:10:05,997 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.507908   0.49209207 0.         0.         0.        ]
2025-06-03 11:10:05,998 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0554889  0.05562794 0.05564477 0.05553098 0.04441698]
2025-06-03 11:10:05,998 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0554889  0.05562794 0.05564477 0.05553098 0.04441698]
2025-06-03 11:10:26,996 INFO --- Epoch 1, Batch 88 (Overall batch 8) ---
2025-06-03 11:10:26,996 INFO --- Epoch 1, Batch 88 (Overall batch 8) ---
2025-06-03 11:10:26,997 INFO Total Loss: 2.9698281288146973
2025-06-03 11:10:26,997 INFO Total Loss: 2.9698281288146973
2025-06-03 11:10:26,998 INFO Causal Consistency Loss: 0.005898026749491692
2025-06-03 11:10:26,998 INFO Causal Consistency Loss: 0.005898026749491692
2025-06-03 11:10:26,998 INFO Accuracy: 0.5938
2025-06-03 11:10:26,998 INFO Accuracy: 0.5938
2025-06-03 11:10:26,999 INFO MCC: 0.2102
2025-06-03 11:10:26,999 INFO MCC: 0.2102
2025-06-03 11:10:27,000 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:27,000 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:27,000 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:27,000 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:27,001 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05646877 0.05549843 0.04504685 0.04387626 0.05549843]
2025-06-03 11:10:27,001 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05646877 0.05549843 0.04504685 0.04387626 0.05549843]
2025-06-03 11:10:45,349 INFO --- Epoch 1, Batch 89 (Overall batch 9) ---
2025-06-03 11:10:45,349 INFO --- Epoch 1, Batch 89 (Overall batch 9) ---
2025-06-03 11:10:45,350 INFO Total Loss: 3.055067300796509
2025-06-03 11:10:45,350 INFO Total Loss: 3.055067300796509
2025-06-03 11:10:45,351 INFO Causal Consistency Loss: 0.004183643497526646
2025-06-03 11:10:45,351 INFO Causal Consistency Loss: 0.004183643497526646
2025-06-03 11:10:45,351 INFO Accuracy: 0.5312
2025-06-03 11:10:45,351 INFO Accuracy: 0.5312
2025-06-03 11:10:45,352 INFO MCC: 0.0422
2025-06-03 11:10:45,352 INFO MCC: 0.0422
2025-06-03 11:10:45,352 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:45,352 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:45,353 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:45,353 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:10:45,354 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04489053 0.05392282 0.05556076 0.05417076 0.03362567]
2025-06-03 11:10:45,354 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04489053 0.05392282 0.05556076 0.05417076 0.03362567]
2025-06-03 11:11:01,401 INFO --- Epoch 1, Batch 90 (Overall batch 10) ---
2025-06-03 11:11:01,401 INFO --- Epoch 1, Batch 90 (Overall batch 10) ---
2025-06-03 11:11:01,402 INFO Total Loss: 2.8623156547546387
2025-06-03 11:11:01,402 INFO Total Loss: 2.8623156547546387
2025-06-03 11:11:01,402 INFO Causal Consistency Loss: 0.004342995118349791
2025-06-03 11:11:01,402 INFO Causal Consistency Loss: 0.004342995118349791
2025-06-03 11:11:01,403 INFO Accuracy: 0.6875
2025-06-03 11:11:01,403 INFO Accuracy: 0.6875
2025-06-03 11:11:01,403 INFO MCC: 0.3651
2025-06-03 11:11:01,403 INFO MCC: 0.3651
2025-06-03 11:11:01,406 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.30341062 0.24541251 0.24451755 0.2066593  0.        ]
2025-06-03 11:11:01,406 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.30341062 0.24541251 0.24451755 0.2066593  0.        ]
2025-06-03 11:11:01,408 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.27891135 0.23346935 0.26025438 0.22736493 0.        ]
2025-06-03 11:11:01,408 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.27891135 0.23346935 0.26025438 0.22736493 0.        ]
2025-06-03 11:11:01,408 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04428998 0.04436022 0.04451165 0.05565516 0.04455821]
2025-06-03 11:11:01,408 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04428998 0.04436022 0.04451165 0.05565516 0.04455821]
2025-06-03 11:11:16,301 INFO --- Epoch 1, Batch 91 (Overall batch 11) ---
2025-06-03 11:11:16,301 INFO --- Epoch 1, Batch 91 (Overall batch 11) ---
2025-06-03 11:11:16,302 INFO Total Loss: 2.973081111907959
2025-06-03 11:11:16,302 INFO Total Loss: 2.973081111907959
2025-06-03 11:11:16,303 INFO Causal Consistency Loss: 0.00440786499530077
2025-06-03 11:11:16,303 INFO Causal Consistency Loss: 0.00440786499530077
2025-06-03 11:11:16,303 INFO Accuracy: 0.6875
2025-06-03 11:11:16,303 INFO Accuracy: 0.6875
2025-06-03 11:11:16,304 INFO MCC: 0.3651
2025-06-03 11:11:16,304 INFO MCC: 0.3651
2025-06-03 11:11:16,305 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.15919687 0.17919965 0.14614026 0.20259579 0.10930011]
2025-06-03 11:11:16,305 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.15919687 0.17919965 0.14614026 0.20259579 0.10930011]
2025-06-03 11:11:16,305 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.14857353 0.15507056 0.14221987 0.20369357 0.18530819]
2025-06-03 11:11:16,305 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.14857353 0.15507056 0.14221987 0.20369357 0.18530819]
2025-06-03 11:11:16,306 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05563756 0.05559077 0.03338423 0.05567617 0.0556058 ]
2025-06-03 11:11:16,306 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05563756 0.05559077 0.03338423 0.05567617 0.0556058 ]
2025-06-03 11:11:31,090 INFO --- Epoch 1, Batch 92 (Overall batch 12) ---
2025-06-03 11:11:31,090 INFO --- Epoch 1, Batch 92 (Overall batch 12) ---
2025-06-03 11:11:31,091 INFO Total Loss: 2.8547112941741943
2025-06-03 11:11:31,091 INFO Total Loss: 2.8547112941741943
2025-06-03 11:11:31,092 INFO Causal Consistency Loss: 0.004198246169835329
2025-06-03 11:11:31,092 INFO Causal Consistency Loss: 0.004198246169835329
2025-06-03 11:11:31,092 INFO Accuracy: 0.6562
2025-06-03 11:11:31,092 INFO Accuracy: 0.6562
2025-06-03 11:11:31,093 INFO MCC: 0.3131
2025-06-03 11:11:31,093 INFO MCC: 0.3131
2025-06-03 11:11:31,094 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:31,094 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:31,095 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:31,095 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:31,095 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04452029 0.05548603 0.04479028 0.04436513 0.05548603]
2025-06-03 11:11:31,095 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04452029 0.05548603 0.04479028 0.04436513 0.05548603]
2025-06-03 11:11:44,431 INFO --- Epoch 1, Batch 93 (Overall batch 13) ---
2025-06-03 11:11:44,431 INFO --- Epoch 1, Batch 93 (Overall batch 13) ---
2025-06-03 11:11:44,433 INFO Total Loss: 3.0176122188568115
2025-06-03 11:11:44,433 INFO Total Loss: 3.0176122188568115
2025-06-03 11:11:44,433 INFO Causal Consistency Loss: 0.003859505755826831
2025-06-03 11:11:44,433 INFO Causal Consistency Loss: 0.003859505755826831
2025-06-03 11:11:44,434 INFO Accuracy: 0.6250
2025-06-03 11:11:44,434 INFO Accuracy: 0.6250
2025-06-03 11:11:44,434 INFO MCC: 0.2471
2025-06-03 11:11:44,434 INFO MCC: 0.2471
2025-06-03 11:11:44,435 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.38707674 0.6129233  0.         0.         0.        ]
2025-06-03 11:11:44,435 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.38707674 0.6129233  0.         0.         0.        ]
2025-06-03 11:11:44,435 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4166648 0.5833352 0.        0.        0.       ]
2025-06-03 11:11:44,435 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4166648 0.5833352 0.        0.        0.       ]
2025-06-03 11:11:44,436 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04525146 0.05535486 0.05456458 0.04467539 0.05579395]
2025-06-03 11:11:44,436 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04525146 0.05535486 0.05456458 0.04467539 0.05579395]
2025-06-03 11:11:57,813 INFO --- Epoch 1, Batch 94 (Overall batch 14) ---
2025-06-03 11:11:57,813 INFO --- Epoch 1, Batch 94 (Overall batch 14) ---
2025-06-03 11:11:57,814 INFO Total Loss: 2.9845688343048096
2025-06-03 11:11:57,814 INFO Total Loss: 2.9845688343048096
2025-06-03 11:11:57,814 INFO Causal Consistency Loss: 0.004030317533761263
2025-06-03 11:11:57,814 INFO Causal Consistency Loss: 0.004030317533761263
2025-06-03 11:11:57,815 INFO Accuracy: 0.5625
2025-06-03 11:11:57,815 INFO Accuracy: 0.5625
2025-06-03 11:11:57,815 INFO MCC: 0.1660
2025-06-03 11:11:57,815 INFO MCC: 0.1660
2025-06-03 11:11:57,816 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:57,816 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:57,816 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:57,816 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:11:57,817 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05525041 0.0444627  0.04450512 0.0556001  0.03345425]
2025-06-03 11:11:57,817 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05525041 0.0444627  0.04450512 0.0556001  0.03345425]
2025-06-03 11:12:11,460 INFO --- Epoch 1, Batch 95 (Overall batch 15) ---
2025-06-03 11:12:11,460 INFO --- Epoch 1, Batch 95 (Overall batch 15) ---
2025-06-03 11:12:11,461 INFO Total Loss: 2.7643542289733887
2025-06-03 11:12:11,461 INFO Total Loss: 2.7643542289733887
2025-06-03 11:12:11,461 INFO Causal Consistency Loss: 0.004669507034122944
2025-06-03 11:12:11,461 INFO Causal Consistency Loss: 0.004669507034122944
2025-06-03 11:12:11,462 INFO Accuracy: 0.7500
2025-06-03 11:12:11,462 INFO Accuracy: 0.7500
2025-06-03 11:12:11,462 INFO MCC: 0.5059
2025-06-03 11:12:11,462 INFO MCC: 0.5059
2025-06-03 11:12:11,463 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:11,463 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:11,464 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:11,464 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:11,464 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05481646 0.05547398 0.05550942 0.05341757 0.05604361]
2025-06-03 11:12:11,464 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05481646 0.05547398 0.05550942 0.05341757 0.05604361]
2025-06-03 11:12:25,657 INFO --- Epoch 1, Batch 96 (Overall batch 16) ---
2025-06-03 11:12:25,657 INFO --- Epoch 1, Batch 96 (Overall batch 16) ---
2025-06-03 11:12:25,658 INFO Total Loss: 2.795245885848999
2025-06-03 11:12:25,658 INFO Total Loss: 2.795245885848999
2025-06-03 11:12:25,658 INFO Causal Consistency Loss: 0.0068642073310911655
2025-06-03 11:12:25,658 INFO Causal Consistency Loss: 0.0068642073310911655
2025-06-03 11:12:25,659 INFO Accuracy: 0.6562
2025-06-03 11:12:25,659 INFO Accuracy: 0.6562
2025-06-03 11:12:25,659 INFO MCC: 0.3077
2025-06-03 11:12:25,659 INFO MCC: 0.3077
2025-06-03 11:12:25,660 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.29673982 0.28768784 0.22068925 0.1948831  0.        ]
2025-06-03 11:12:25,660 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.29673982 0.28768784 0.22068925 0.1948831  0.        ]
2025-06-03 11:12:25,661 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.26126635 0.2769879  0.26159504 0.20015062 0.        ]
2025-06-03 11:12:25,661 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.26126635 0.2769879  0.26159504 0.20015062 0.        ]
2025-06-03 11:12:25,663 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05557893 0.05547699 0.05571174 0.05574667 0.05544428]
2025-06-03 11:12:25,663 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05557893 0.05547699 0.05571174 0.05574667 0.05544428]
2025-06-03 11:12:39,473 INFO --- Epoch 1, Batch 97 (Overall batch 17) ---
2025-06-03 11:12:39,473 INFO --- Epoch 1, Batch 97 (Overall batch 17) ---
2025-06-03 11:12:39,474 INFO Total Loss: 2.7055823802948
2025-06-03 11:12:39,474 INFO Total Loss: 2.7055823802948
2025-06-03 11:12:39,474 INFO Causal Consistency Loss: 0.00301723787561059
2025-06-03 11:12:39,474 INFO Causal Consistency Loss: 0.00301723787561059
2025-06-03 11:12:39,475 INFO Accuracy: 0.7188
2025-06-03 11:12:39,475 INFO Accuracy: 0.7188
2025-06-03 11:12:39,475 INFO MCC: 0.4497
2025-06-03 11:12:39,475 INFO MCC: 0.4497
2025-06-03 11:12:39,476 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:39,476 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:39,476 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:39,476 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:39,477 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05591472 0.04450305 0.05568916 0.05568916 0.04456317]
2025-06-03 11:12:39,477 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05591472 0.04450305 0.05568916 0.05568916 0.04456317]
2025-06-03 11:12:54,518 INFO --- Epoch 1, Batch 98 (Overall batch 18) ---
2025-06-03 11:12:54,518 INFO --- Epoch 1, Batch 98 (Overall batch 18) ---
2025-06-03 11:12:54,519 INFO Total Loss: 2.821415901184082
2025-06-03 11:12:54,519 INFO Total Loss: 2.821415901184082
2025-06-03 11:12:54,520 INFO Causal Consistency Loss: 0.004640988539904356
2025-06-03 11:12:54,520 INFO Causal Consistency Loss: 0.004640988539904356
2025-06-03 11:12:54,520 INFO Accuracy: 0.5938
2025-06-03 11:12:54,520 INFO Accuracy: 0.5938
2025-06-03 11:12:54,521 INFO MCC: 0.1189
2025-06-03 11:12:54,521 INFO MCC: 0.1189
2025-06-03 11:12:54,521 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:54,521 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:54,522 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:54,522 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:12:54,522 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05456439 0.04400828 0.05414172 0.04382467 0.05473294]
2025-06-03 11:12:54,522 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05456439 0.04400828 0.05414172 0.04382467 0.05473294]
2025-06-03 11:13:07,702 INFO --- Epoch 1, Batch 99 (Overall batch 19) ---
2025-06-03 11:13:07,702 INFO --- Epoch 1, Batch 99 (Overall batch 19) ---
2025-06-03 11:13:07,703 INFO Total Loss: 2.7131006717681885
2025-06-03 11:13:07,703 INFO Total Loss: 2.7131006717681885
2025-06-03 11:13:07,704 INFO Causal Consistency Loss: 0.005470535717904568
2025-06-03 11:13:07,704 INFO Causal Consistency Loss: 0.005470535717904568
2025-06-03 11:13:07,705 INFO Accuracy: 0.8438
2025-06-03 11:13:07,705 INFO Accuracy: 0.8438
2025-06-03 11:13:07,705 INFO MCC: 0.6814
2025-06-03 11:13:07,705 INFO MCC: 0.6814
2025-06-03 11:13:07,706 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.56829685 0.43170312 0.         0.         0.        ]
2025-06-03 11:13:07,706 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.56829685 0.43170312 0.         0.         0.        ]
2025-06-03 11:13:07,707 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5164562 0.4835438 0.        0.        0.       ]
2025-06-03 11:13:07,707 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5164562 0.4835438 0.        0.        0.       ]
2025-06-03 11:13:07,707 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04428198 0.04495292 0.05537776 0.04436042 0.04472197]
2025-06-03 11:13:07,707 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04428198 0.04495292 0.05537776 0.04436042 0.04472197]
2025-06-03 11:13:21,089 INFO --- Epoch 1, Batch 100 (Overall batch 20) ---
2025-06-03 11:13:21,089 INFO --- Epoch 1, Batch 100 (Overall batch 20) ---
2025-06-03 11:13:21,090 INFO Total Loss: 2.659510612487793
2025-06-03 11:13:21,090 INFO Total Loss: 2.659510612487793
2025-06-03 11:13:21,090 INFO Causal Consistency Loss: 0.005107816308736801
2025-06-03 11:13:21,090 INFO Causal Consistency Loss: 0.005107816308736801
2025-06-03 11:13:21,091 INFO Accuracy: 0.7188
2025-06-03 11:13:21,091 INFO Accuracy: 0.7188
2025-06-03 11:13:21,091 INFO MCC: 0.4384
2025-06-03 11:13:21,091 INFO MCC: 0.4384
2025-06-03 11:13:21,092 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:13:21,092 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:13:21,092 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:13:21,092 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:13:21,093 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05534622 0.04455853 0.05473565 0.05521424 0.05569889]
2025-06-03 11:13:21,093 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05534622 0.04455853 0.05473565 0.05521424 0.05569889]
2025-06-03 11:13:24,575 INFO Reached max_batches_to_test (20). Stopping training for test.
2025-06-03 11:13:24,575 INFO Reached max_batches_to_test (20). Stopping training for test.
2025-06-03 11:13:25,416 INFO Saved training metrics plot to results\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp_training_metrics.png
2025-06-03 11:13:25,416 INFO Saved training metrics plot to results\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp_training_metrics.png
2025-06-03 11:13:26,041 INFO Model all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp saved at step 100
2025-06-03 11:13:26,041 INFO Model all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp saved at step 100
2025-06-03 11:13:26,042 INFO Start dev phase...
2025-06-03 11:13:26,042 INFO Start dev phase...
2025-06-03 11:15:05,523 INFO 	Eval, eval loss: 2.7090532779693604, acc: 0.492378
2025-06-03 11:15:05,523 INFO 	Eval, eval loss: 2.7090532779693604, acc: 0.492378
2025-06-03 11:15:05,643 INFO Noise-aware loss test finished.
2025-06-03 11:15:05,643 INFO Noise-aware loss test finished.
2025-06-03 11:56:50,637 INFO Starting noise-aware loss test...
2025-06-03 11:56:50,637 INFO Starting noise-aware loss test...
2025-06-03 11:56:50,638 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 11:56:50,638 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 11:56:50,640 INFO Start graph assembling...
2025-06-03 11:56:50,640 INFO Start graph assembling...
2025-06-03 11:56:53,537 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:56:53,537 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:56:53,538 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:56:53,538 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 11:57:11,233 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 11:57:11,233 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 11:57:12,542 INFO Word table init: done!
2025-06-03 11:57:12,542 INFO Word table init: done!
2025-06-03 11:57:13,204 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 11:57:13,204 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 11:57:13,204 INFO Starting Epoch 1/10...
2025-06-03 11:57:13,204 INFO Starting Epoch 1/10...
2025-06-03 11:57:34,961 INFO --- Epoch 1, Batch 101 (Overall batch 1) ---
2025-06-03 11:57:34,961 INFO --- Epoch 1, Batch 101 (Overall batch 1) ---
2025-06-03 11:57:34,963 INFO Total Loss: 2.6489720344543457
2025-06-03 11:57:34,963 INFO Total Loss: 2.6489720344543457
2025-06-03 11:57:34,964 INFO Causal Consistency Loss: 0.0034447191283106804
2025-06-03 11:57:34,964 INFO Causal Consistency Loss: 0.0034447191283106804
2025-06-03 11:57:34,964 INFO Accuracy: 0.5938
2025-06-03 11:57:34,964 INFO Accuracy: 0.5938
2025-06-03 11:57:34,964 INFO MCC: 0.1879
2025-06-03 11:57:34,964 INFO MCC: 0.1879
2025-06-03 11:57:34,965 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.05008961 0.05535467 0.04447762 0.06493451 0.05201171]
2025-06-03 11:57:34,965 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.05008961 0.05535467 0.04447762 0.06493451 0.05201171]
2025-06-03 11:57:34,966 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.05172649 0.05397537 0.04449979 0.05270558 0.05606456]
2025-06-03 11:57:34,966 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.05172649 0.05397537 0.04449979 0.05270558 0.05606456]
2025-06-03 11:57:34,966 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05553261 0.04389721 0.0549903  0.0535531  0.05520403]
2025-06-03 11:57:34,966 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05553261 0.04389721 0.0549903  0.0535531  0.05520403]
2025-06-03 11:58:06,876 INFO --- Epoch 1, Batch 102 (Overall batch 2) ---
2025-06-03 11:58:06,876 INFO --- Epoch 1, Batch 102 (Overall batch 2) ---
2025-06-03 11:58:06,877 INFO Total Loss: 2.800567865371704
2025-06-03 11:58:06,877 INFO Total Loss: 2.800567865371704
2025-06-03 11:58:06,878 INFO Causal Consistency Loss: 0.0029742668848484755
2025-06-03 11:58:06,878 INFO Causal Consistency Loss: 0.0029742668848484755
2025-06-03 11:58:06,878 INFO Accuracy: 0.5312
2025-06-03 11:58:06,878 INFO Accuracy: 0.5312
2025-06-03 11:58:06,878 INFO MCC: 0.1078
2025-06-03 11:58:06,878 INFO MCC: 0.1078
2025-06-03 11:58:06,879 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:06,879 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:06,879 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:06,879 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:06,880 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05536386 0.05539722 0.05530949 0.05604816 0.04443117]
2025-06-03 11:58:06,880 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05536386 0.05539722 0.05530949 0.05604816 0.04443117]
2025-06-03 11:58:28,367 INFO --- Epoch 1, Batch 103 (Overall batch 3) ---
2025-06-03 11:58:28,367 INFO --- Epoch 1, Batch 103 (Overall batch 3) ---
2025-06-03 11:58:28,367 INFO Total Loss: 2.75258207321167
2025-06-03 11:58:28,367 INFO Total Loss: 2.75258207321167
2025-06-03 11:58:28,368 INFO Causal Consistency Loss: 0.0037364393938332796
2025-06-03 11:58:28,368 INFO Causal Consistency Loss: 0.0037364393938332796
2025-06-03 11:58:28,368 INFO Accuracy: 0.6250
2025-06-03 11:58:28,368 INFO Accuracy: 0.6250
2025-06-03 11:58:28,368 INFO MCC: 0.2277
2025-06-03 11:58:28,368 INFO MCC: 0.2277
2025-06-03 11:58:28,369 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:28,369 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:28,369 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:28,369 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:28,370 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05554933 0.05545638 0.05550811 0.05545638 0.05545638]
2025-06-03 11:58:28,370 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05554933 0.05545638 0.05550811 0.05545638 0.05545638]
2025-06-03 11:58:50,610 INFO --- Epoch 1, Batch 104 (Overall batch 4) ---
2025-06-03 11:58:50,610 INFO --- Epoch 1, Batch 104 (Overall batch 4) ---
2025-06-03 11:58:50,611 INFO Total Loss: 2.6049928665161133
2025-06-03 11:58:50,611 INFO Total Loss: 2.6049928665161133
2025-06-03 11:58:50,611 INFO Causal Consistency Loss: 0.004886911250650883
2025-06-03 11:58:50,611 INFO Causal Consistency Loss: 0.004886911250650883
2025-06-03 11:58:50,611 INFO Accuracy: 0.7188
2025-06-03 11:58:50,611 INFO Accuracy: 0.7188
2025-06-03 11:58:50,612 INFO MCC: 0.4339
2025-06-03 11:58:50,612 INFO MCC: 0.4339
2025-06-03 11:58:50,612 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:50,612 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:50,613 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:50,613 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:58:50,613 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05693386 0.04753092 0.05608847 0.04225475 0.03416175]
2025-06-03 11:58:50,613 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05693386 0.04753092 0.05608847 0.04225475 0.03416175]
2025-06-03 11:59:12,994 INFO --- Epoch 1, Batch 105 (Overall batch 5) ---
2025-06-03 11:59:12,994 INFO --- Epoch 1, Batch 105 (Overall batch 5) ---
2025-06-03 11:59:12,994 INFO Total Loss: 2.8049285411834717
2025-06-03 11:59:12,994 INFO Total Loss: 2.8049285411834717
2025-06-03 11:59:12,995 INFO Causal Consistency Loss: 0.0037052666302770376
2025-06-03 11:59:12,995 INFO Causal Consistency Loss: 0.0037052666302770376
2025-06-03 11:59:12,995 INFO Accuracy: 0.6562
2025-06-03 11:59:12,995 INFO Accuracy: 0.6562
2025-06-03 11:59:12,995 INFO MCC: 0.3131
2025-06-03 11:59:12,995 INFO MCC: 0.3131
2025-06-03 11:59:12,996 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4745907  0.52540934 0.         0.         0.        ]
2025-06-03 11:59:12,996 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4745907  0.52540934 0.         0.         0.        ]
2025-06-03 11:59:12,996 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5108225  0.48917755 0.         0.         0.        ]
2025-06-03 11:59:12,996 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5108225  0.48917755 0.         0.         0.        ]
2025-06-03 11:59:12,997 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04414725 0.0555207  0.04449025 0.0556352  0.04427186]
2025-06-03 11:59:12,997 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04414725 0.0555207  0.04449025 0.0556352  0.04427186]
2025-06-03 11:59:29,367 INFO --- Epoch 1, Batch 106 (Overall batch 6) ---
2025-06-03 11:59:29,367 INFO --- Epoch 1, Batch 106 (Overall batch 6) ---
2025-06-03 11:59:29,368 INFO Total Loss: 2.629265308380127
2025-06-03 11:59:29,368 INFO Total Loss: 2.629265308380127
2025-06-03 11:59:29,368 INFO Causal Consistency Loss: 0.0037214981857687235
2025-06-03 11:59:29,368 INFO Causal Consistency Loss: 0.0037214981857687235
2025-06-03 11:59:29,369 INFO Accuracy: 0.6562
2025-06-03 11:59:29,369 INFO Accuracy: 0.6562
2025-06-03 11:59:29,369 INFO MCC: 0.3631
2025-06-03 11:59:29,369 INFO MCC: 0.3631
2025-06-03 11:59:29,370 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:59:29,370 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:59:29,371 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:59:29,371 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 11:59:29,372 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04422765 0.04408116 0.044356   0.05624372 0.05288647]
2025-06-03 11:59:29,372 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04422765 0.04408116 0.044356   0.05624372 0.05288647]
2025-06-03 11:59:48,426 INFO --- Epoch 1, Batch 107 (Overall batch 7) ---
2025-06-03 11:59:48,426 INFO --- Epoch 1, Batch 107 (Overall batch 7) ---
2025-06-03 11:59:48,427 INFO Total Loss: 2.6539509296417236
2025-06-03 11:59:48,427 INFO Total Loss: 2.6539509296417236
2025-06-03 11:59:48,427 INFO Causal Consistency Loss: 0.0031963870860636234
2025-06-03 11:59:48,427 INFO Causal Consistency Loss: 0.0031963870860636234
2025-06-03 11:59:48,428 INFO Accuracy: 0.6875
2025-06-03 11:59:48,428 INFO Accuracy: 0.6875
2025-06-03 11:59:48,428 INFO MCC: 0.2981
2025-06-03 11:59:48,428 INFO MCC: 0.2981
2025-06-03 11:59:48,429 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.3113037  0.31460878 0.3740875  0.         0.        ]
2025-06-03 11:59:48,429 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.3113037  0.31460878 0.3740875  0.         0.        ]
2025-06-03 11:59:48,429 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.3505773  0.33478698 0.31463575 0.         0.        ]
2025-06-03 11:59:48,429 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.3505773  0.33478698 0.31463575 0.         0.        ]
2025-06-03 11:59:48,430 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04449912 0.05605527 0.05549169 0.0554823  0.04433466]
2025-06-03 11:59:48,430 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04449912 0.05605527 0.05549169 0.0554823  0.04433466]
2025-06-03 12:00:05,695 INFO --- Epoch 1, Batch 108 (Overall batch 8) ---
2025-06-03 12:00:05,695 INFO --- Epoch 1, Batch 108 (Overall batch 8) ---
2025-06-03 12:00:05,695 INFO Total Loss: 2.315110921859741
2025-06-03 12:00:05,695 INFO Total Loss: 2.315110921859741
2025-06-03 12:00:05,695 INFO Causal Consistency Loss: 0.002809249795973301
2025-06-03 12:00:05,695 INFO Causal Consistency Loss: 0.002809249795973301
2025-06-03 12:00:05,697 INFO Accuracy: 0.8125
2025-06-03 12:00:05,697 INFO Accuracy: 0.8125
2025-06-03 12:00:05,697 INFO MCC: 0.6386
2025-06-03 12:00:05,697 INFO MCC: 0.6386
2025-06-03 12:00:05,698 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.510337 0.489663 0.       0.       0.      ]
2025-06-03 12:00:05,698 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.510337 0.489663 0.       0.       0.      ]
2025-06-03 12:00:05,698 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4854598  0.51454014 0.         0.         0.        ]
2025-06-03 12:00:05,698 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4854598  0.51454014 0.         0.         0.        ]
2025-06-03 12:00:05,699 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05532308 0.04400717 0.05574644 0.05556434 0.05544002]
2025-06-03 12:00:05,699 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05532308 0.04400717 0.05574644 0.05556434 0.05544002]
2025-06-03 12:00:21,938 INFO --- Epoch 1, Batch 109 (Overall batch 9) ---
2025-06-03 12:00:21,938 INFO --- Epoch 1, Batch 109 (Overall batch 9) ---
2025-06-03 12:00:21,939 INFO Total Loss: 2.5021135807037354
2025-06-03 12:00:21,939 INFO Total Loss: 2.5021135807037354
2025-06-03 12:00:21,940 INFO Causal Consistency Loss: 0.003434934187680483
2025-06-03 12:00:21,940 INFO Causal Consistency Loss: 0.003434934187680483
2025-06-03 12:00:21,940 INFO Accuracy: 0.6875
2025-06-03 12:00:21,940 INFO Accuracy: 0.6875
2025-06-03 12:00:21,940 INFO MCC: 0.3522
2025-06-03 12:00:21,940 INFO MCC: 0.3522
2025-06-03 12:00:21,941 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.56589067 0.4341094  0.         0.         0.        ]
2025-06-03 12:00:21,941 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.56589067 0.4341094  0.         0.         0.        ]
2025-06-03 12:00:21,941 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5235549  0.47644514 0.         0.         0.        ]
2025-06-03 12:00:21,941 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5235549  0.47644514 0.         0.         0.        ]
2025-06-03 12:00:21,942 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04517523 0.05459058 0.05536935 0.05536935 0.04308589]
2025-06-03 12:00:21,942 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04517523 0.05459058 0.05536935 0.05536935 0.04308589]
2025-06-03 12:00:38,577 INFO --- Epoch 1, Batch 110 (Overall batch 10) ---
2025-06-03 12:00:38,577 INFO --- Epoch 1, Batch 110 (Overall batch 10) ---
2025-06-03 12:00:38,577 INFO Total Loss: 2.848517656326294
2025-06-03 12:00:38,577 INFO Total Loss: 2.848517656326294
2025-06-03 12:00:38,578 INFO Causal Consistency Loss: 0.0038046783301979303
2025-06-03 12:00:38,578 INFO Causal Consistency Loss: 0.0038046783301979303
2025-06-03 12:00:38,578 INFO Accuracy: 0.5000
2025-06-03 12:00:38,578 INFO Accuracy: 0.5000
2025-06-03 12:00:38,578 INFO MCC: 0.0325
2025-06-03 12:00:38,578 INFO MCC: 0.0325
2025-06-03 12:00:38,579 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.16227114 0.17159826 0.16330329 0.15786952 0.18279496]
2025-06-03 12:00:38,579 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.16227114 0.17159826 0.16330329 0.15786952 0.18279496]
2025-06-03 12:00:38,580 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.16518618 0.15456407 0.1705055  0.16231062 0.18189436]
2025-06-03 12:00:38,580 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.16518618 0.15456407 0.1705055  0.16231062 0.18189436]
2025-06-03 12:00:38,580 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04445572 0.0554533  0.04460439 0.03336617 0.03327892]
2025-06-03 12:00:38,580 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04445572 0.0554533  0.04460439 0.03336617 0.03327892]
2025-06-03 12:00:52,331 INFO --- Epoch 1, Batch 111 (Overall batch 11) ---
2025-06-03 12:00:52,331 INFO --- Epoch 1, Batch 111 (Overall batch 11) ---
2025-06-03 12:00:52,332 INFO Total Loss: 2.405181407928467
2025-06-03 12:00:52,332 INFO Total Loss: 2.405181407928467
2025-06-03 12:00:52,333 INFO Causal Consistency Loss: 0.0034503061324357986
2025-06-03 12:00:52,333 INFO Causal Consistency Loss: 0.0034503061324357986
2025-06-03 12:00:52,333 INFO Accuracy: 0.8125
2025-06-03 12:00:52,333 INFO Accuracy: 0.8125
2025-06-03 12:00:52,333 INFO MCC: 0.6181
2025-06-03 12:00:52,333 INFO MCC: 0.6181
2025-06-03 12:00:52,334 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:00:52,334 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:00:52,334 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:00:52,334 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:00:52,335 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0556769  0.0546622  0.05514556 0.05581201 0.05514556]
2025-06-03 12:00:52,335 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0556769  0.0546622  0.05514556 0.05581201 0.05514556]
2025-06-03 12:01:07,077 INFO --- Epoch 1, Batch 112 (Overall batch 12) ---
2025-06-03 12:01:07,077 INFO --- Epoch 1, Batch 112 (Overall batch 12) ---
2025-06-03 12:01:07,077 INFO Total Loss: 2.5517725944519043
2025-06-03 12:01:07,077 INFO Total Loss: 2.5517725944519043
2025-06-03 12:01:07,078 INFO Causal Consistency Loss: 0.0032750575337558985
2025-06-03 12:01:07,078 INFO Causal Consistency Loss: 0.0032750575337558985
2025-06-03 12:01:07,078 INFO Accuracy: 0.5312
2025-06-03 12:01:07,078 INFO Accuracy: 0.5312
2025-06-03 12:01:07,079 INFO MCC: 0.0552
2025-06-03 12:01:07,079 INFO MCC: 0.0552
2025-06-03 12:01:07,079 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.45692804 0.5430719  0.         0.         0.        ]
2025-06-03 12:01:07,079 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.45692804 0.5430719  0.         0.         0.        ]
2025-06-03 12:01:07,080 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4961143 0.5038857 0.        0.        0.       ]
2025-06-03 12:01:07,080 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4961143 0.5038857 0.        0.        0.       ]
2025-06-03 12:01:07,080 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05547167 0.05562042 0.04443261 0.05555421 0.05555421]
2025-06-03 12:01:07,080 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05547167 0.05562042 0.04443261 0.05555421 0.05555421]
2025-06-03 12:01:21,166 INFO --- Epoch 1, Batch 113 (Overall batch 13) ---
2025-06-03 12:01:21,166 INFO --- Epoch 1, Batch 113 (Overall batch 13) ---
2025-06-03 12:01:21,167 INFO Total Loss: 2.4694197177886963
2025-06-03 12:01:21,167 INFO Total Loss: 2.4694197177886963
2025-06-03 12:01:21,167 INFO Causal Consistency Loss: 0.0034213971812278032
2025-06-03 12:01:21,167 INFO Causal Consistency Loss: 0.0034213971812278032
2025-06-03 12:01:21,167 INFO Accuracy: 0.7188
2025-06-03 12:01:21,167 INFO Accuracy: 0.7188
2025-06-03 12:01:21,168 INFO MCC: 0.4730
2025-06-03 12:01:21,168 INFO MCC: 0.4730
2025-06-03 12:01:21,168 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.569304   0.43069604 0.         0.         0.        ]
2025-06-03 12:01:21,168 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.569304   0.43069604 0.         0.         0.        ]
2025-06-03 12:01:21,169 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.53169286 0.46830708 0.         0.         0.        ]
2025-06-03 12:01:21,169 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.53169286 0.46830708 0.         0.         0.        ]
2025-06-03 12:01:21,170 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.03323534 0.05579308 0.04449997 0.04409598 0.04425715]
2025-06-03 12:01:21,170 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.03323534 0.05579308 0.04449997 0.04409598 0.04425715]
2025-06-03 12:01:38,784 INFO --- Epoch 1, Batch 114 (Overall batch 14) ---
2025-06-03 12:01:38,784 INFO --- Epoch 1, Batch 114 (Overall batch 14) ---
2025-06-03 12:01:38,784 INFO Total Loss: 2.5951666831970215
2025-06-03 12:01:38,784 INFO Total Loss: 2.5951666831970215
2025-06-03 12:01:38,785 INFO Causal Consistency Loss: 0.004108794033527374
2025-06-03 12:01:38,785 INFO Causal Consistency Loss: 0.004108794033527374
2025-06-03 12:01:38,785 INFO Accuracy: 0.5938
2025-06-03 12:01:38,785 INFO Accuracy: 0.5938
2025-06-03 12:01:38,785 INFO MCC: 0.1879
2025-06-03 12:01:38,785 INFO MCC: 0.1879
2025-06-03 12:01:38,786 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:01:38,786 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:01:38,786 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:01:38,786 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:01:38,787 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04485608 0.04394228 0.0549066  0.05182279 0.05613558]
2025-06-03 12:01:38,787 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04485608 0.04394228 0.0549066  0.05182279 0.05613558]
2025-06-03 12:01:53,964 INFO --- Epoch 1, Batch 115 (Overall batch 15) ---
2025-06-03 12:01:53,964 INFO --- Epoch 1, Batch 115 (Overall batch 15) ---
2025-06-03 12:01:53,966 INFO Total Loss: 2.375420331954956
2025-06-03 12:01:53,966 INFO Total Loss: 2.375420331954956
2025-06-03 12:01:53,966 INFO Causal Consistency Loss: 0.003675660118460655
2025-06-03 12:01:53,966 INFO Causal Consistency Loss: 0.003675660118460655
2025-06-03 12:01:53,966 INFO Accuracy: 0.7188
2025-06-03 12:01:53,966 INFO Accuracy: 0.7188
2025-06-03 12:01:53,966 INFO MCC: 0.4032
2025-06-03 12:01:53,966 INFO MCC: 0.4032
2025-06-03 12:01:53,967 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.10310093 0.08534655 0.10324533 0.07543424 0.07318508]
2025-06-03 12:01:53,967 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.10310093 0.08534655 0.10324533 0.07543424 0.07318508]
2025-06-03 12:01:53,968 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.09114828 0.08432538 0.09547797 0.09430034 0.08449051]
2025-06-03 12:01:53,968 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.09114828 0.08432538 0.09547797 0.09430034 0.08449051]
2025-06-03 12:01:53,968 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0550759  0.03315518 0.03355106 0.03341305 0.05585748]
2025-06-03 12:01:53,968 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0550759  0.03315518 0.03355106 0.03341305 0.05585748]
2025-06-03 12:02:07,742 INFO --- Epoch 1, Batch 116 (Overall batch 16) ---
2025-06-03 12:02:07,742 INFO --- Epoch 1, Batch 116 (Overall batch 16) ---
2025-06-03 12:02:07,743 INFO Total Loss: 2.4865036010742188
2025-06-03 12:02:07,743 INFO Total Loss: 2.4865036010742188
2025-06-03 12:02:07,744 INFO Causal Consistency Loss: 0.0027447568718343973
2025-06-03 12:02:07,744 INFO Causal Consistency Loss: 0.0027447568718343973
2025-06-03 12:02:07,744 INFO Accuracy: 0.5000
2025-06-03 12:02:07,744 INFO Accuracy: 0.5000
2025-06-03 12:02:07,744 INFO MCC: 0.0000
2025-06-03 12:02:07,744 INFO MCC: 0.0000
2025-06-03 12:02:07,745 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:07,745 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:07,745 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:07,745 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:07,746 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05548523 0.0335354  0.04443147 0.04435637 0.05551153]
2025-06-03 12:02:07,746 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05548523 0.0335354  0.04443147 0.04435637 0.05551153]
2025-06-03 12:02:22,157 INFO --- Epoch 1, Batch 117 (Overall batch 17) ---
2025-06-03 12:02:22,157 INFO --- Epoch 1, Batch 117 (Overall batch 17) ---
2025-06-03 12:02:22,158 INFO Total Loss: 2.535494089126587
2025-06-03 12:02:22,158 INFO Total Loss: 2.535494089126587
2025-06-03 12:02:22,158 INFO Causal Consistency Loss: 0.0026491221506148577
2025-06-03 12:02:22,158 INFO Causal Consistency Loss: 0.0026491221506148577
2025-06-03 12:02:22,159 INFO Accuracy: 0.5312
2025-06-03 12:02:22,159 INFO Accuracy: 0.5312
2025-06-03 12:02:22,159 INFO MCC: 0.0882
2025-06-03 12:02:22,159 INFO MCC: 0.0882
2025-06-03 12:02:22,159 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.24299812 0.27913913 0.25168237 0.22618037 0.        ]
2025-06-03 12:02:22,159 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.24299812 0.27913913 0.25168237 0.22618037 0.        ]
2025-06-03 12:02:22,160 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.25541753 0.24341217 0.2536213  0.24754901 0.        ]
2025-06-03 12:02:22,160 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.25541753 0.24341217 0.2536213  0.24754901 0.        ]
2025-06-03 12:02:22,160 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05498289 0.05631564 0.05609726 0.05601327 0.0557089 ]
2025-06-03 12:02:22,160 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05498289 0.05631564 0.05609726 0.05601327 0.0557089 ]
2025-06-03 12:02:35,773 INFO --- Epoch 1, Batch 118 (Overall batch 18) ---
2025-06-03 12:02:35,773 INFO --- Epoch 1, Batch 118 (Overall batch 18) ---
2025-06-03 12:02:35,774 INFO Total Loss: 2.359923839569092
2025-06-03 12:02:35,774 INFO Total Loss: 2.359923839569092
2025-06-03 12:02:35,774 INFO Causal Consistency Loss: 0.003580138087272644
2025-06-03 12:02:35,774 INFO Causal Consistency Loss: 0.003580138087272644
2025-06-03 12:02:35,774 INFO Accuracy: 0.6562
2025-06-03 12:02:35,774 INFO Accuracy: 0.6562
2025-06-03 12:02:35,775 INFO MCC: 0.3131
2025-06-03 12:02:35,775 INFO MCC: 0.3131
2025-06-03 12:02:35,775 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.23386075 0.23471488 0.27013236 0.261292   0.        ]
2025-06-03 12:02:35,775 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.23386075 0.23471488 0.27013236 0.261292   0.        ]
2025-06-03 12:02:35,776 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.25926316 0.24549605 0.251615   0.24362579 0.        ]
2025-06-03 12:02:35,776 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.25926316 0.24549605 0.251615   0.24362579 0.        ]
2025-06-03 12:02:35,776 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05785812 0.055588   0.04429218 0.04484973 0.05716219]
2025-06-03 12:02:35,776 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05785812 0.055588   0.04429218 0.04484973 0.05716219]
2025-06-03 12:02:49,687 INFO --- Epoch 1, Batch 119 (Overall batch 19) ---
2025-06-03 12:02:49,687 INFO --- Epoch 1, Batch 119 (Overall batch 19) ---
2025-06-03 12:02:49,688 INFO Total Loss: 2.4890060424804688
2025-06-03 12:02:49,688 INFO Total Loss: 2.4890060424804688
2025-06-03 12:02:49,688 INFO Causal Consistency Loss: 0.0029420179780572653
2025-06-03 12:02:49,688 INFO Causal Consistency Loss: 0.0029420179780572653
2025-06-03 12:02:49,689 INFO Accuracy: 0.6875
2025-06-03 12:02:49,689 INFO Accuracy: 0.6875
2025-06-03 12:02:49,689 INFO MCC: 0.3578
2025-06-03 12:02:49,689 INFO MCC: 0.3578
2025-06-03 12:02:49,689 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:49,689 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:49,690 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:49,690 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:02:49,690 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05561389 0.05480565 0.03313198 0.05577859 0.0445402 ]
2025-06-03 12:02:49,690 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05561389 0.05480565 0.03313198 0.05577859 0.0445402 ]
2025-06-03 12:03:03,280 INFO --- Epoch 1, Batch 120 (Overall batch 20) ---
2025-06-03 12:03:03,280 INFO --- Epoch 1, Batch 120 (Overall batch 20) ---
2025-06-03 12:03:03,281 INFO Total Loss: 2.6024484634399414
2025-06-03 12:03:03,281 INFO Total Loss: 2.6024484634399414
2025-06-03 12:03:03,282 INFO Causal Consistency Loss: 0.0026131318882107735
2025-06-03 12:03:03,282 INFO Causal Consistency Loss: 0.0026131318882107735
2025-06-03 12:03:03,282 INFO Accuracy: 0.6250
2025-06-03 12:03:03,282 INFO Accuracy: 0.6250
2025-06-03 12:03:03,282 INFO MCC: 0.2500
2025-06-03 12:03:03,282 INFO MCC: 0.2500
2025-06-03 12:03:03,283 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.2740359  0.2678815  0.2166792  0.24140337 0.        ]
2025-06-03 12:03:03,283 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.2740359  0.2678815  0.2166792  0.24140337 0.        ]
2025-06-03 12:03:03,283 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.261712   0.24158902 0.25867155 0.23802738 0.        ]
2025-06-03 12:03:03,283 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.261712   0.24158902 0.25867155 0.23802738 0.        ]
2025-06-03 12:03:03,284 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0558675  0.04461398 0.05549876 0.0445491  0.05536952]
2025-06-03 12:03:03,284 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0558675  0.04461398 0.05549876 0.0445491  0.05536952]
2025-06-03 12:03:07,306 INFO Reached max_batches_to_test (20). Stopping training for test.
2025-06-03 12:03:07,306 INFO Reached max_batches_to_test (20). Stopping training for test.
2025-06-03 12:03:07,788 INFO Saved training metrics plot to results\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp_training_metrics.png
2025-06-03 12:03:07,788 INFO Saved training metrics plot to results\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp_training_metrics.png
2025-06-03 12:03:08,358 INFO Model all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp saved at step 120
2025-06-03 12:03:08,358 INFO Model all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp saved at step 120
2025-06-03 12:03:08,359 INFO Start dev phase...
2025-06-03 12:03:08,359 INFO Start dev phase...
2025-06-03 12:04:43,316 INFO 	Eval, eval loss: 2.4511659145355225, acc: 0.525915
2025-06-03 12:04:43,316 INFO 	Eval, eval loss: 2.4511659145355225, acc: 0.525915
2025-06-03 12:04:43,441 INFO Noise-aware loss test finished.
2025-06-03 12:04:43,441 INFO Noise-aware loss test finished.
2025-06-03 12:11:01,582 INFO Starting noise-aware loss test...
2025-06-03 12:11:01,582 INFO Starting noise-aware loss test...
2025-06-03 12:11:01,583 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:11:01,583 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:11:01,585 INFO Start graph assembling...
2025-06-03 12:11:01,585 INFO Start graph assembling...
2025-06-03 12:11:04,488 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:11:04,488 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:11:04,488 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:11:04,488 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:11:22,277 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 12:11:22,277 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 12:11:23,270 INFO Word table init: done!
2025-06-03 12:11:23,270 INFO Word table init: done!
2025-06-03 12:11:23,932 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 12:11:23,932 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 12:11:23,933 INFO Starting Epoch 1/10...
2025-06-03 12:11:23,933 INFO Starting Epoch 1/10...
2025-06-03 12:19:20,610 INFO Starting noise-aware loss test...
2025-06-03 12:19:20,610 INFO Starting noise-aware loss test...
2025-06-03 12:19:20,611 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:19:20,611 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:19:20,613 INFO Start graph assembling...
2025-06-03 12:19:20,613 INFO Start graph assembling...
2025-06-03 12:19:23,483 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:19:23,483 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:19:23,485 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:19:23,485 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:20:16,934 INFO Starting noise-aware loss test...
2025-06-03 12:20:16,934 INFO Starting noise-aware loss test...
2025-06-03 12:20:16,934 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:20:16,934 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:20:16,936 INFO Start graph assembling...
2025-06-03 12:20:16,936 INFO Start graph assembling...
2025-06-03 12:20:19,813 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:20:19,813 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:20:19,814 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:20:19,814 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:20:37,791 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 12:20:37,791 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 12:20:38,736 INFO Word table init: done!
2025-06-03 12:20:38,736 INFO Word table init: done!
2025-06-03 12:20:39,390 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 12:20:39,390 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 12:20:39,391 INFO Starting Epoch 1/10...
2025-06-03 12:20:39,391 INFO Starting Epoch 1/10...
2025-06-03 12:23:03,284 INFO Starting noise-aware loss test...
2025-06-03 12:23:03,285 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 12:23:03,287 INFO Start graph assembling...
2025-06-03 12:23:06,127 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:23:06,128 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 12:23:24,672 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 12:23:25,628 INFO Word table init: done!
2025-06-03 12:23:26,298 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 12:23:26,298 INFO Starting Epoch 1/10...
2025-06-03 12:24:09,552 INFO --- Epoch 1, Batch 121 (Overall batch 1) ---
2025-06-03 12:24:09,556 INFO Total Loss: 2.5769214630126953
2025-06-03 12:24:09,557 INFO Causal Consistency Loss: 0.0018748764414340258
2025-06-03 12:24:09,558 INFO Accuracy: 0.5312
2025-06-03 12:24:09,558 INFO MCC: 0.0164
2025-06-03 12:24:09,559 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:24:09,560 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 12:24:09,560 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04178692 0.05998812 0.03097965 0.05563766 0.05563766]
2025-06-03 17:02:03,198 INFO Starting noise-aware loss test...
2025-06-03 17:02:03,199 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-03 17:02:03,205 INFO Start graph assembling...
2025-06-03 17:02:06,507 INFO Ensured checkpoint directory exists: ./checkpoints_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 17:02:06,509 INFO Ensured graph directory exists: ./graphs_tx_lf\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp
2025-06-03 17:02:24,568 INFO ASSEMBLE: word table #replacement: 19482
2025-06-03 17:02:26,176 INFO Word table init: done!
2025-06-03 17:02:26,912 INFO Model: all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp, session restored!
2025-06-03 17:02:26,913 INFO Starting Epoch 1/10...
2025-06-03 17:03:00,986 INFO --- Epoch 1, Batch 121 (Overall batch 1) ---
2025-06-03 17:03:00,987 INFO Total Loss: 2.4204037189483643
2025-06-03 17:03:00,988 INFO Causal Consistency Loss: 0.002538356464356184
2025-06-03 17:03:00,988 INFO Accuracy: 0.7188
2025-06-03 17:03:00,989 INFO MCC: 0.3769
2025-06-03 17:03:00,991 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:03:00,993 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:03:00,995 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05621231 0.05546697 0.05667507 0.03328003 0.04443904]
2025-06-03 17:03:27,182 INFO --- Epoch 1, Batch 122 (Overall batch 2) ---
2025-06-03 17:03:27,182 INFO Total Loss: 2.565789222717285
2025-06-03 17:03:27,183 INFO Causal Consistency Loss: 0.0017693033441901207
2025-06-03 17:03:27,183 INFO Accuracy: 0.6250
2025-06-03 17:03:27,183 INFO MCC: 0.2208
2025-06-03 17:03:27,184 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.25565794 0.26373798 0.27164322 0.20896089 0.        ]
2025-06-03 17:03:27,185 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.26920125 0.23333225 0.24872695 0.24873957 0.        ]
2025-06-03 17:03:27,185 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0451694  0.04423471 0.04496703 0.03378641 0.04394607]
2025-06-03 17:03:48,147 INFO --- Epoch 1, Batch 123 (Overall batch 3) ---
2025-06-03 17:03:48,148 INFO Total Loss: 2.46684193611145
2025-06-03 17:03:48,149 INFO Causal Consistency Loss: 0.002174053806811571
2025-06-03 17:03:48,149 INFO Accuracy: 0.5312
2025-06-03 17:03:48,149 INFO MCC: 0.1078
2025-06-03 17:03:48,150 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.53755563 0.46244437 0.         0.         0.        ]
2025-06-03 17:03:48,151 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.55013937 0.4498606  0.         0.         0.        ]
2025-06-03 17:03:48,151 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0427018  0.05651623 0.0465223  0.03455984 0.05409354]
2025-06-03 17:04:04,364 INFO --- Epoch 1, Batch 124 (Overall batch 4) ---
2025-06-03 17:04:04,364 INFO Total Loss: 2.650622606277466
2025-06-03 17:04:04,365 INFO Causal Consistency Loss: 0.002761205891147256
2025-06-03 17:04:04,365 INFO Accuracy: 0.6875
2025-06-03 17:04:04,365 INFO MCC: 0.3578
2025-06-03 17:04:04,366 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:04:04,367 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:04:04,367 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04438305 0.04465954 0.0554366  0.04440569 0.05542018]
2025-06-03 17:04:25,066 INFO --- Epoch 1, Batch 125 (Overall batch 5) ---
2025-06-03 17:04:25,067 INFO Total Loss: 2.424666166305542
2025-06-03 17:04:25,068 INFO Causal Consistency Loss: 0.0012089618248865008
2025-06-03 17:04:25,068 INFO Accuracy: 0.6250
2025-06-03 17:04:25,068 INFO MCC: 0.2471
2025-06-03 17:04:25,069 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:04:25,070 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:04:25,070 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04651724 0.05420177 0.04454963 0.05561739 0.05561739]
2025-06-03 17:04:45,267 INFO --- Epoch 1, Batch 126 (Overall batch 6) ---
2025-06-03 17:04:45,268 INFO Total Loss: 2.421271324157715
2025-06-03 17:04:45,268 INFO Causal Consistency Loss: 0.0018821216654032469
2025-06-03 17:04:45,268 INFO Accuracy: 0.5312
2025-06-03 17:04:45,269 INFO MCC: 0.0882
2025-06-03 17:04:45,269 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.4930947 0.5069053 0.        0.        0.       ]
2025-06-03 17:04:45,270 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.4879822 0.5120178 0.        0.        0.       ]
2025-06-03 17:04:45,270 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04505508 0.03322555 0.05509798 0.05616557 0.04470081]
2025-06-03 17:05:00,917 INFO --- Epoch 1, Batch 127 (Overall batch 7) ---
2025-06-03 17:05:00,918 INFO Total Loss: 2.4020817279815674
2025-06-03 17:05:00,918 INFO Causal Consistency Loss: 0.003187305061146617
2025-06-03 17:05:00,918 INFO Accuracy: 0.6875
2025-06-03 17:05:00,919 INFO MCC: 0.3651
2025-06-03 17:05:00,920 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.25782308 0.24439411 0.243345   0.25443783 0.        ]
2025-06-03 17:05:00,920 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.24589084 0.26612613 0.231106   0.25687706 0.        ]
2025-06-03 17:05:00,921 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0558078  0.05567064 0.05579825 0.03332018 0.05546621]
2025-06-03 17:05:18,185 INFO --- Epoch 1, Batch 128 (Overall batch 8) ---
2025-06-03 17:05:18,186 INFO Total Loss: 2.5305137634277344
2025-06-03 17:05:18,187 INFO Causal Consistency Loss: 0.002041314262896776
2025-06-03 17:05:18,187 INFO Accuracy: 0.5312
2025-06-03 17:05:18,187 INFO MCC: 0.0636
2025-06-03 17:05:18,188 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.50263005 0.49736992 0.         0.         0.        ]
2025-06-03 17:05:18,188 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5019946 0.4980054 0.        0.        0.       ]
2025-06-03 17:05:18,189 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04484039 0.04380296 0.04431507 0.05595164 0.03350225]
2025-06-03 17:05:34,561 INFO --- Epoch 1, Batch 129 (Overall batch 9) ---
2025-06-03 17:05:34,563 INFO Total Loss: 2.47265625
2025-06-03 17:05:34,563 INFO Causal Consistency Loss: 0.002298352774232626
2025-06-03 17:05:34,564 INFO Accuracy: 0.5312
2025-06-03 17:05:34,564 INFO MCC: 0.0626
2025-06-03 17:05:34,565 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.5549517 0.4450483 0.        0.        0.       ]
2025-06-03 17:05:34,565 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.5047773  0.49522275 0.         0.         0.        ]
2025-06-03 17:05:34,566 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05545192 0.05535886 0.05568598 0.0441675  0.05578057]
2025-06-03 17:05:49,629 INFO --- Epoch 1, Batch 130 (Overall batch 10) ---
2025-06-03 17:05:49,630 INFO Total Loss: 2.2826266288757324
2025-06-03 17:05:49,630 INFO Causal Consistency Loss: 0.0015602785861119628
2025-06-03 17:05:49,631 INFO Accuracy: 0.6250
2025-06-03 17:05:49,631 INFO MCC: 0.2582
2025-06-03 17:05:49,632 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:05:49,633 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:05:49,633 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05566967 0.05558809 0.05558809 0.05558809 0.05541191]
2025-06-03 17:06:08,968 INFO --- Epoch 1, Batch 131 (Overall batch 11) ---
2025-06-03 17:06:08,969 INFO Total Loss: 2.28389310836792
2025-06-03 17:06:08,970 INFO Causal Consistency Loss: 0.0019768690690398216
2025-06-03 17:06:08,970 INFO Accuracy: 0.7500
2025-06-03 17:06:08,971 INFO MCC: 0.4880
2025-06-03 17:06:08,971 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:08,972 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:08,972 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04451535 0.04547303 0.04424272 0.04454409 0.05431067]
2025-06-03 17:06:23,529 INFO --- Epoch 1, Batch 132 (Overall batch 12) ---
2025-06-03 17:06:23,529 INFO Total Loss: 2.3395676612854004
2025-06-03 17:06:23,530 INFO Causal Consistency Loss: 0.0021922574378550053
2025-06-03 17:06:23,530 INFO Accuracy: 0.7188
2025-06-03 17:06:23,531 INFO MCC: 0.4249
2025-06-03 17:06:23,532 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.05434625 0.04452122 0.04979565 0.05808376 0.04449657]
2025-06-03 17:06:23,532 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.05441177 0.0505781  0.05388467 0.05128538 0.04664436]
2025-06-03 17:06:23,532 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05565066 0.05556166 0.05560021 0.033386   0.04425496]
2025-06-03 17:06:39,311 INFO --- Epoch 1, Batch 133 (Overall batch 13) ---
2025-06-03 17:06:39,312 INFO Total Loss: 2.6481564044952393
2025-06-03 17:06:39,313 INFO Causal Consistency Loss: 0.0020101768895983696
2025-06-03 17:06:39,313 INFO Accuracy: 0.5000
2025-06-03 17:06:39,313 INFO MCC: 0.0039
2025-06-03 17:06:39,314 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:39,315 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:39,315 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04488725 0.0442249  0.05535405 0.05535405 0.05606104]
2025-06-03 17:06:53,762 INFO --- Epoch 1, Batch 134 (Overall batch 14) ---
2025-06-03 17:06:53,762 INFO Total Loss: 2.3965961933135986
2025-06-03 17:06:53,763 INFO Causal Consistency Loss: 0.002309501636773348
2025-06-03 17:06:53,763 INFO Accuracy: 0.5938
2025-06-03 17:06:53,763 INFO MCC: 0.1972
2025-06-03 17:06:53,763 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:53,765 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:06:53,765 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05534161 0.05563734 0.04455835 0.05524417 0.05563734]
2025-06-03 17:07:08,302 INFO --- Epoch 1, Batch 135 (Overall batch 15) ---
2025-06-03 17:07:08,303 INFO Total Loss: 2.4481008052825928
2025-06-03 17:07:08,303 INFO Causal Consistency Loss: 0.0017958058742806315
2025-06-03 17:07:08,305 INFO Accuracy: 0.6250
2025-06-03 17:07:08,305 INFO MCC: 0.2481
2025-06-03 17:07:08,305 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.30660865 0.34602553 0.34736586 0.         0.        ]
2025-06-03 17:07:08,306 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.32708916 0.36050743 0.31240338 0.         0.        ]
2025-06-03 17:07:08,307 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05470683 0.0465284  0.04654049 0.03214742 0.0434627 ]
2025-06-03 17:07:23,738 INFO --- Epoch 1, Batch 136 (Overall batch 16) ---
2025-06-03 17:07:23,740 INFO Total Loss: 2.379413604736328
2025-06-03 17:07:23,740 INFO Causal Consistency Loss: 0.0018984873313456774
2025-06-03 17:07:23,741 INFO Accuracy: 0.6250
2025-06-03 17:07:23,741 INFO MCC: 0.2500
2025-06-03 17:07:23,742 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.05525175 0.05571685 0.06327663 0.0486631  0.06049502]
2025-06-03 17:07:23,742 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.0579694  0.05465465 0.06249822 0.05760423 0.06125099]
2025-06-03 17:07:23,743 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05588297 0.05559919 0.05512489 0.05560896 0.0562245 ]
2025-06-03 17:07:38,216 INFO --- Epoch 1, Batch 137 (Overall batch 17) ---
2025-06-03 17:07:38,216 INFO Total Loss: 2.400470733642578
2025-06-03 17:07:38,217 INFO Causal Consistency Loss: 0.0022347159683704376
2025-06-03 17:07:38,217 INFO Accuracy: 0.5625
2025-06-03 17:07:38,218 INFO MCC: 0.1395
2025-06-03 17:07:38,218 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.32264373 0.2929723  0.38438398 0.         0.        ]
2025-06-03 17:07:38,219 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.3302998 0.3296995 0.3400007 0.        0.       ]
2025-06-03 17:07:38,219 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.05010688 0.03489813 0.0451534  0.05488139 0.04726813]
2025-06-03 17:07:52,455 INFO --- Epoch 1, Batch 138 (Overall batch 18) ---
2025-06-03 17:07:52,456 INFO Total Loss: 2.383720874786377
2025-06-03 17:07:52,456 INFO Causal Consistency Loss: 0.002059951424598694
2025-06-03 17:07:52,457 INFO Accuracy: 0.6250
2025-06-03 17:07:52,457 INFO MCC: 0.2582
2025-06-03 17:07:52,457 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.12063044 0.11447845 0.12092829 0.1084716  0.12401855]
2025-06-03 17:07:52,458 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.11008024 0.11489979 0.1172616  0.11245065 0.11597955]
2025-06-03 17:07:52,459 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.11434779 0.08689272 0.05289295 0.06772511 0.04486769]
2025-06-03 17:08:05,974 INFO --- Epoch 1, Batch 139 (Overall batch 19) ---
2025-06-03 17:08:05,975 INFO Total Loss: 2.4295477867126465
2025-06-03 17:08:05,976 INFO Causal Consistency Loss: 0.0017514607170596719
2025-06-03 17:08:05,976 INFO Accuracy: 0.7500
2025-06-03 17:08:05,976 INFO MCC: 0.5220
2025-06-03 17:08:05,977 INFO Alpha_i (sample 0, day 0, first 5 msgs): [0.25204232 0.24826472 0.26567653 0.23401645 0.        ]
2025-06-03 17:08:05,977 INFO Omega_i (sample 0, day 0, first 5 msgs): [0.2562132  0.24366497 0.25930455 0.24081728 0.        ]
2025-06-03 17:08:05,978 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.0334668  0.04359307 0.05564101 0.05593671 0.04548066]
2025-06-03 17:08:19,268 INFO --- Epoch 1, Batch 140 (Overall batch 20) ---
2025-06-03 17:08:19,269 INFO Total Loss: 2.3131532669067383
2025-06-03 17:08:19,269 INFO Causal Consistency Loss: 0.001827130327001214
2025-06-03 17:08:19,270 INFO Accuracy: 0.7188
2025-06-03 17:08:19,271 INFO MCC: 0.3637
2025-06-03 17:08:19,271 INFO Alpha_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:08:19,272 INFO Omega_i (sample 0, day 0, first 5 msgs): [1. 0. 0. 0. 0.]
2025-06-03 17:08:19,272 INFO Attention Weights (first item in batch, first day if applicable, first 5 items):
[0.04492109 0.05524453 0.04399013 0.05524453 0.04434398]
2025-06-03 17:08:21,205 INFO Reached max_batches_to_test (20). Stopping training for test.
2025-06-03 17:08:23,041 INFO Saved training metrics plot to results\all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp_training_metrics.png
2025-06-03 17:08:26,142 INFO Model all_days-5.msgs-20-words-30_word_embed-glove.vmd_in-hedge_alpha-0.5.anneal-0.005.rec-zh_batch-32.opt-adam.lr-0.001-drop-0.3-cell-gru-tmp saved at step 140
2025-06-03 17:08:26,143 INFO Start dev phase...
2025-06-03 17:10:03,872 INFO 	Eval, eval loss: 2.395338773727417, acc: 0.535061
2025-06-03 17:10:04,002 INFO Noise-aware loss test finished.
2025-06-16 05:01:34,823 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:01:34,826 INFO Start graph assembling...
2025-06-16 05:01:36,553 INFO use_dual_path_srl: True
2025-06-16 05:01:36,554 INFO variant_type: hedge
2025-06-16 05:01:36,556 INFO use_enhanced_srl: True
2025-06-16 05:01:36,556 INFO Enhanced SRL enabled from config
2025-06-16 05:01:36,557 INFO Dual-path SRL automatically enabled for Enhanced SRL
2025-06-16 05:01:36,557 INFO SRL condition: True = True and True
2025-06-16 05:01:36,558 INFO Using enhanced dual-path SRL architecture
2025-06-16 05:01:36,558 INFO Creating Enhanced Dual-Path SRL module...
2025-06-16 05:01:36,602 INFO Text to price predictor input shape: [None, 5, 100], target_dim: 3
2025-06-16 05:01:36,607 INFO 3D input reshaped to: [batch*time, features] = [-1, 100]
2025-06-16 05:01:36,627 INFO Predictions reshaped to: [batch, time, target_dim] = [Tensor("enhanced_dual_path_srl/text_driven_pathway/text_to_price_predictor/strided_slice:0", shape=(), dtype=int32, device=/device:GPU:0), seq_length, 3]
2025-06-16 05:01:36,933 INFO Enhanced Dual-Path SRL module created successfully.
2025-06-16 05:01:36,935 INFO Enhanced dual-path output found - using h_enhanced_dual_path
2025-06-16 05:01:36,935 INFO Final VMD input tensor: enhanced_dual_path_srl/pathway_fusion/add_6:0 with shape (?, 5, 100)
2025-06-16 05:01:37,128 INFO Using Temporal Attention (TAP)
2025-06-16 05:01:37,172 INFO Building Adaptive Temporal Attention (ATA) module and Final Loss...
2025-06-16 05:01:37,173 INFO Ca-TSU enabled: True
2025-06-16 05:01:37,173 INFO Enhanced SRL enabled: True
2025-06-16 05:01:37,174 INFO Noise-Aware Loss enabled: True
2025-06-16 05:01:37,188 INFO Using Generative Base Loss (Likelihood + KL Objective): Tensor("generative_ata_loss/Mean:0", shape=(), dtype=float32, device=/device:GPU:0)
2025-06-16 05:01:37,189 INFO Adding auxiliary enhancement losses...
2025-06-16 05:01:37,193 INFO Creating Noise-Aware Reliability Weights for Contrastive Loss...
2025-06-16 05:01:37,204 INFO Using Noise-Aware Contrastive Loss as SRL component with weight: 0.14678259732696614
2025-06-16 05:01:37,206 INFO Creating Causal Consistency Loss component (KL Divergence)...
2025-06-16 05:01:37,206 INFO Causal Loss KL Divergence: P(alpha_i) shape=(?, 5, 20), Q(omega_i) shape=<unknown>
2025-06-16 05:01:37,216 INFO Using KL-based Causal Consistency Loss with weight: 0.14149911285154604
2025-06-16 05:01:37,217 INFO Final loss after all components: Tensor("causal_consistency_component_kl/add_causal_loss_kl:0", dtype=float32, device=/device:GPU:0)
2025-06-16 05:01:37,218 INFO Final loss components being optimized: ['base_generative_objective', 'noise_aware_loss_raw', 'causal_consistency_loss_raw']
2025-06-16 05:01:39,562 INFO Graph assembled successfully.
2025-06-16 05:01:57,783 INFO ASSEMBLE: word table #replacement: 19482
2025-06-16 05:01:59,079 INFO Word table initialized.
2025-06-16 05:01:59,080 INFO Model: all-hedge-gru_c3d38f61, starting new session.
2025-06-16 05:06:40,440 INFO 	iter: 20, batch loss: 1.2097033262252808, batch acc: 0.020508
2025-06-16 05:07:30,318 INFO Epoch 1 finished. Checkpoint saved.
2025-06-16 05:11:20,660 INFO 	iter: 40, batch loss: 0.9644396901130676, batch acc: 0.013672
2025-06-16 05:13:01,740 INFO Epoch 2 finished. Checkpoint saved.
2025-06-16 05:16:01,430 INFO 	iter: 60, batch loss: 0.8074793219566345, batch acc: 0.020508
2025-06-16 05:18:29,104 INFO Epoch 3 finished. Checkpoint saved.
2025-06-16 05:18:29,105 INFO Generating diagnostic plots...
2025-06-16 05:18:31,342 INFO Diagnostic plots saved to results/model_test/component_plots
2025-06-16 05:19:50,699 INFO Completed dev evaluation: size=420.0, acc=0.4548
2025-06-16 05:19:50,701 INFO 	Eval, eval loss: 0.8074036836624146, acc: 0.454762, mcc: -0.032582
2025-06-16 05:19:50,892 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:50,895 INFO Start graph assembling...
2025-06-16 05:19:50,934 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:50,937 INFO Start graph assembling...
2025-06-16 05:19:50,966 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:50,971 INFO Start graph assembling...
2025-06-16 05:19:50,994 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:50,997 INFO Start graph assembling...
2025-06-16 05:19:51,026 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,029 INFO Start graph assembling...
2025-06-16 05:19:51,054 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,059 INFO Start graph assembling...
2025-06-16 05:19:51,085 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,089 INFO Start graph assembling...
2025-06-16 05:19:51,116 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,120 INFO Start graph assembling...
2025-06-16 05:19:51,150 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,155 INFO Start graph assembling...
2025-06-16 05:19:51,189 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,194 INFO Start graph assembling...
2025-06-16 05:19:51,218 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,222 INFO Start graph assembling...
2025-06-16 05:19:51,245 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,248 INFO Start graph assembling...
2025-06-16 05:19:51,273 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,276 INFO Start graph assembling...
2025-06-16 05:19:51,299 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,302 INFO Start graph assembling...
2025-06-16 05:19:51,328 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,332 INFO Start graph assembling...
2025-06-16 05:19:51,356 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,359 INFO Start graph assembling...
2025-06-16 05:19:51,381 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,384 INFO Start graph assembling...
2025-06-16 05:19:51,405 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,408 INFO Start graph assembling...
2025-06-16 05:19:51,432 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,435 INFO Start graph assembling...
2025-06-16 05:19:51,456 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,459 INFO Start graph assembling...
2025-06-16 05:19:51,479 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,481 INFO Start graph assembling...
2025-06-16 05:19:51,501 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,504 INFO Start graph assembling...
2025-06-16 05:19:51,524 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,527 INFO Start graph assembling...
2025-06-16 05:19:51,545 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,548 INFO Start graph assembling...
2025-06-16 05:19:51,568 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,570 INFO Start graph assembling...
2025-06-16 05:19:51,590 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,593 INFO Start graph assembling...
2025-06-16 05:19:51,772 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,773 INFO Start graph assembling...
2025-06-16 05:19:51,791 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,793 INFO Start graph assembling...
2025-06-16 05:19:51,812 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,814 INFO Start graph assembling...
2025-06-16 05:19:51,833 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,835 INFO Start graph assembling...
2025-06-16 05:19:51,854 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,856 INFO Start graph assembling...
2025-06-16 05:19:51,874 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,876 INFO Start graph assembling...
2025-06-16 05:19:51,894 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,897 INFO Start graph assembling...
2025-06-16 05:19:51,918 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,921 INFO Start graph assembling...
2025-06-16 05:19:51,945 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,948 INFO Start graph assembling...
2025-06-16 05:19:51,969 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,973 INFO Start graph assembling...
2025-06-16 05:19:51,993 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:51,996 INFO Start graph assembling...
2025-06-16 05:19:52,018 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,020 INFO Start graph assembling...
2025-06-16 05:19:52,040 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,043 INFO Start graph assembling...
2025-06-16 05:19:52,066 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,070 INFO Start graph assembling...
2025-06-16 05:19:52,094 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,097 INFO Start graph assembling...
2025-06-16 05:19:52,122 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,127 INFO Start graph assembling...
2025-06-16 05:19:52,152 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,156 INFO Start graph assembling...
2025-06-16 05:19:52,176 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,179 INFO Start graph assembling...
2025-06-16 05:19:52,200 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,203 INFO Start graph assembling...
2025-06-16 05:19:52,222 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,225 INFO Start graph assembling...
2025-06-16 05:19:52,244 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,246 INFO Start graph assembling...
2025-06-16 05:19:52,266 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,269 INFO Start graph assembling...
2025-06-16 05:19:52,288 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:19:52,290 INFO Start graph assembling...
2025-06-16 05:25:36,858 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:25:36,861 INFO Start graph assembling...
2025-06-16 05:25:38,741 INFO use_dual_path_srl: True
2025-06-16 05:25:38,742 INFO variant_type: hedge
2025-06-16 05:25:38,742 INFO use_enhanced_srl: True
2025-06-16 05:25:38,743 INFO Enhanced SRL enabled from config
2025-06-16 05:25:38,744 INFO Dual-path SRL automatically enabled for Enhanced SRL
2025-06-16 05:25:38,744 INFO SRL condition: True = True and True
2025-06-16 05:25:38,744 INFO Using enhanced dual-path SRL architecture
2025-06-16 05:25:38,745 INFO Creating Enhanced Dual-Path SRL module...
2025-06-16 05:25:38,790 INFO Text to price predictor input shape: [None, 5, 100], target_dim: 3
2025-06-16 05:25:38,795 INFO 3D input reshaped to: [batch*time, features] = [-1, 100]
2025-06-16 05:25:38,815 INFO Predictions reshaped to: [batch, time, target_dim] = [Tensor("enhanced_dual_path_srl/text_driven_pathway/text_to_price_predictor/strided_slice:0", shape=(), dtype=int32, device=/device:GPU:0), seq_length, 3]
2025-06-16 05:25:39,131 INFO Enhanced Dual-Path SRL module created successfully.
2025-06-16 05:25:39,132 INFO Enhanced dual-path output found - using h_enhanced_dual_path
2025-06-16 05:25:39,132 INFO Final VMD input tensor: enhanced_dual_path_srl/pathway_fusion/add_6:0 with shape (?, 5, 100)
2025-06-16 05:25:39,343 INFO Using Temporal Attention (TAP)
2025-06-16 05:25:39,387 INFO Building Adaptive Temporal Attention (ATA) module and Final Loss...
2025-06-16 05:25:39,388 INFO Ca-TSU enabled: True
2025-06-16 05:25:39,389 INFO Enhanced SRL enabled: True
2025-06-16 05:25:39,389 INFO Noise-Aware Loss enabled: True
2025-06-16 05:25:39,404 INFO Using Generative Base Loss (Likelihood + KL Objective): Tensor("generative_ata_loss/Mean:0", shape=(), dtype=float32, device=/device:GPU:0)
2025-06-16 05:25:39,405 INFO Adding auxiliary enhancement losses...
2025-06-16 05:25:39,409 INFO Creating Noise-Aware Reliability Weights for Contrastive Loss...
2025-06-16 05:25:39,420 INFO Using Noise-Aware Contrastive Loss as SRL component with weight: 0.15315703994294622
2025-06-16 05:25:39,422 INFO Creating Causal Consistency Loss component (KL Divergence)...
2025-06-16 05:25:39,423 INFO Causal Loss KL Divergence: P(alpha_i) shape=(?, 5, 20), Q(omega_i) shape=<unknown>
2025-06-16 05:25:39,433 INFO Using KL-based Causal Consistency Loss with weight: 0.6183063804607332
2025-06-16 05:25:39,434 INFO Final loss after all components: Tensor("causal_consistency_component_kl/add_causal_loss_kl:0", dtype=float32, device=/device:GPU:0)
2025-06-16 05:25:39,435 INFO Final loss components being optimized: ['base_generative_objective', 'noise_aware_loss_raw', 'causal_consistency_loss_raw']
2025-06-16 05:25:41,891 INFO Graph assembled successfully.
2025-06-16 05:25:59,978 INFO ASSEMBLE: word table #replacement: 19482
2025-06-16 05:26:01,424 INFO Word table initialized.
2025-06-16 05:26:01,426 INFO Model: all-hedge-gru_6852cc19, starting new session.
2025-06-16 05:30:44,198 INFO 	iter: 20, batch loss: 1.3794220685958862, batch acc: 0.656250
2025-06-16 05:31:34,108 INFO Epoch 1 finished. Checkpoint saved.
2025-06-16 05:35:24,454 INFO 	iter: 40, batch loss: 0.994213879108429, batch acc: 0.593750
2025-06-16 05:37:05,056 INFO Epoch 2 finished. Checkpoint saved.
2025-06-16 05:40:03,707 INFO 	iter: 60, batch loss: 0.7729192972183228, batch acc: 0.656250
2025-06-16 05:42:34,220 INFO Epoch 3 finished. Checkpoint saved.
2025-06-16 05:42:34,221 INFO Generating diagnostic plots...
2025-06-16 05:42:36,469 INFO Diagnostic plots saved to results/model_test/component_plots
2025-06-16 05:43:59,223 INFO Completed dev evaluation: size=420.0, acc=0.5310
2025-06-16 05:43:59,224 INFO 	Eval, eval loss: 0.7507549524307251, acc: 0.530952, mcc: 0.022574
2025-06-16 05:43:59,417 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 05:43:59,419 INFO Start graph assembling...
2025-06-16 05:44:01,657 INFO use_dual_path_srl: True
2025-06-16 05:44:01,658 INFO variant_type: hedge
2025-06-16 05:44:01,658 INFO use_enhanced_srl: True
2025-06-16 05:44:01,659 INFO Enhanced SRL enabled from config
2025-06-16 05:44:01,659 INFO Dual-path SRL automatically enabled for Enhanced SRL
2025-06-16 05:44:01,660 INFO SRL condition: True = True and True
2025-06-16 05:44:01,660 INFO Using enhanced dual-path SRL architecture
2025-06-16 05:44:01,660 INFO Creating Enhanced Dual-Path SRL module...
2025-06-16 05:44:01,704 INFO Text to price predictor input shape: [None, 5, 100], target_dim: 3
2025-06-16 05:44:01,710 INFO 3D input reshaped to: [batch*time, features] = [-1, 100]
2025-06-16 05:44:01,732 INFO Predictions reshaped to: [batch, time, target_dim] = [Tensor("enhanced_dual_path_srl/text_driven_pathway/text_to_price_predictor/strided_slice:0", shape=(), dtype=int32, device=/device:GPU:0), seq_length, 3]
2025-06-16 05:44:02,060 INFO Enhanced Dual-Path SRL module created successfully.
2025-06-16 05:44:02,061 INFO Enhanced dual-path output found - using h_enhanced_dual_path
2025-06-16 05:44:02,062 INFO Final VMD input tensor: enhanced_dual_path_srl/pathway_fusion/add_6:0 with shape (?, 5, 100)
2025-06-16 05:44:02,267 INFO Using Temporal Attention (TAP)
2025-06-16 05:44:02,314 INFO Building Adaptive Temporal Attention (ATA) module and Final Loss...
2025-06-16 05:44:02,314 INFO Ca-TSU enabled: True
2025-06-16 05:44:02,315 INFO Enhanced SRL enabled: True
2025-06-16 05:44:02,316 INFO Noise-Aware Loss enabled: True
2025-06-16 05:44:02,330 INFO Using Generative Base Loss (Likelihood + KL Objective): Tensor("generative_ata_loss/Mean:0", shape=(), dtype=float32, device=/device:GPU:0)
2025-06-16 05:44:02,331 INFO Adding auxiliary enhancement losses...
2025-06-16 05:44:02,336 INFO Creating Noise-Aware Reliability Weights for Contrastive Loss...
2025-06-16 05:44:02,348 INFO Using Noise-Aware Contrastive Loss as SRL component with weight: 0.4038800300098692
2025-06-16 05:44:02,349 INFO Creating Causal Consistency Loss component (KL Divergence)...
2025-06-16 05:44:02,350 INFO Causal Loss KL Divergence: P(alpha_i) shape=(?, 5, 20), Q(omega_i) shape=<unknown>
2025-06-16 05:44:02,360 INFO Using KL-based Causal Consistency Loss with weight: 0.4025476029891924
2025-06-16 05:44:02,360 INFO Final loss after all components: Tensor("causal_consistency_component_kl/add_causal_loss_kl:0", dtype=float32, device=/device:GPU:0)
2025-06-16 05:44:02,361 INFO Final loss components being optimized: ['base_generative_objective', 'noise_aware_loss_raw', 'causal_consistency_loss_raw']
2025-06-16 05:44:04,844 INFO Graph assembled successfully.
2025-06-16 05:44:23,057 INFO ASSEMBLE: word table #replacement: 19482
2025-06-16 05:44:23,843 INFO Word table initialized.
2025-06-16 05:44:23,844 INFO Model: all-hedge-gru_20194b07, starting new session.
2025-06-16 05:49:08,535 INFO 	iter: 20, batch loss: 1.300557255744934, batch acc: 0.625000
2025-06-16 05:50:00,058 INFO Epoch 1 finished. Checkpoint saved.
2025-06-16 05:53:49,041 INFO 	iter: 40, batch loss: 1.0455877780914307, batch acc: 0.468750
2025-06-16 05:55:28,965 INFO Epoch 2 finished. Checkpoint saved.
2025-06-16 05:58:26,438 INFO 	iter: 60, batch loss: 0.8638330698013306, batch acc: 0.593750
2025-06-16 06:00:57,608 INFO Epoch 3 finished. Checkpoint saved.
2025-06-16 06:00:57,609 INFO Generating diagnostic plots...
2025-06-16 06:00:59,816 INFO Diagnostic plots saved to results/model_test/component_plots
2025-06-16 06:02:20,955 INFO Completed dev evaluation: size=420.0, acc=0.4833
2025-06-16 06:02:20,957 INFO 	Eval, eval loss: 0.8231965899467468, acc: 0.483333, mcc: -0.026428
2025-06-16 06:02:21,141 INFO INIT: #stock: 88, #vocab+1: 29867
2025-06-16 06:02:21,143 INFO Start graph assembling...
2025-06-16 06:02:23,291 INFO use_dual_path_srl: True
2025-06-16 06:02:23,292 INFO variant_type: hedge
2025-06-16 06:02:23,292 INFO use_enhanced_srl: True
2025-06-16 06:02:23,293 INFO Enhanced SRL enabled from config
2025-06-16 06:02:23,293 INFO Dual-path SRL automatically enabled for Enhanced SRL
2025-06-16 06:02:23,293 INFO SRL condition: True = True and True
2025-06-16 06:02:23,294 INFO Using enhanced dual-path SRL architecture
2025-06-16 06:02:23,294 INFO Creating Enhanced Dual-Path SRL module...
2025-06-16 06:02:23,340 INFO Text to price predictor input shape: [None, 5, 100], target_dim: 3
2025-06-16 06:02:23,345 INFO 3D input reshaped to: [batch*time, features] = [-1, 100]
2025-06-16 06:02:23,368 INFO Predictions reshaped to: [batch, time, target_dim] = [Tensor("enhanced_dual_path_srl/text_driven_pathway/text_to_price_predictor/strided_slice:0", shape=(), dtype=int32, device=/device:GPU:0), seq_length, 3]
2025-06-16 06:02:23,696 INFO Enhanced Dual-Path SRL module created successfully.
2025-06-16 06:02:23,697 INFO Enhanced dual-path output found - using h_enhanced_dual_path
2025-06-16 06:02:23,698 INFO Final VMD input tensor: enhanced_dual_path_srl/pathway_fusion/add_6:0 with shape (?, 5, 100)
2025-06-16 06:02:23,901 INFO Using Temporal Attention (TAP)
2025-06-16 06:02:23,947 INFO Building Adaptive Temporal Attention (ATA) module and Final Loss...
2025-06-16 06:02:23,948 INFO Ca-TSU enabled: True
2025-06-16 06:02:23,948 INFO Enhanced SRL enabled: True
2025-06-16 06:02:23,949 INFO Noise-Aware Loss enabled: True
2025-06-16 06:02:23,964 INFO Using Generative Base Loss (Likelihood + KL Objective): Tensor("generative_ata_loss/Mean:0", shape=(), dtype=float32, device=/device:GPU:0)
2025-06-16 06:02:23,964 INFO Adding auxiliary enhancement losses...
2025-06-16 06:02:23,968 INFO Creating Noise-Aware Reliability Weights for Contrastive Loss...
2025-06-16 06:02:23,980 INFO Using Noise-Aware Contrastive Loss as SRL component with weight: 0.18082405748438934
2025-06-16 06:02:23,982 INFO Creating Causal Consistency Loss component (KL Divergence)...
2025-06-16 06:02:23,983 INFO Causal Loss KL Divergence: P(alpha_i) shape=(?, 5, 20), Q(omega_i) shape=<unknown>
2025-06-16 06:02:23,993 INFO Using KL-based Causal Consistency Loss with weight: 0.24237599698962034
2025-06-16 06:02:23,994 INFO Final loss after all components: Tensor("causal_consistency_component_kl/add_causal_loss_kl:0", dtype=float32, device=/device:GPU:0)
2025-06-16 06:02:23,994 INFO Final loss components being optimized: ['base_generative_objective', 'noise_aware_loss_raw', 'causal_consistency_loss_raw']
2025-06-16 06:02:26,506 INFO Graph assembled successfully.
2025-06-16 06:02:44,465 INFO ASSEMBLE: word table #replacement: 19482
2025-06-16 06:02:45,222 INFO Word table initialized.
2025-06-16 06:02:45,223 INFO Model: all-hedge-gru_a52b0337, starting new session.
